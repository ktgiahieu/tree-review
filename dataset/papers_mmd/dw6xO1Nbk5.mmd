# Generalization in Neural Operator: Irregular Domains, Orthogonal Basis, and Super-Resolution

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Neural operators (NOs) have become popular for learning partial differential equation (PDE) operators. As a mapping between infinite-dimensional function spaces, each layer of NO contains a kernel operator and a linear transform, followed by nonlinear activation. NO can accurately simulate the operator and conduct super-resolution, i.e., train and test on grids with different resolutions. Despite its success, NO's design of kernel operator, choice of grids, the capability of generalization and super-resolution, and applicability to general problems on irregular domains are poorly understood. To this end, we systematically analyze NOs from a unified perspective, considering the orthogonal bases in their kernel operators. This analysis facilitates a better understanding and enhancement of NOs in the following: (1) Generalization bounds of NOs, (2) Construction of NOs on arbitrary domains, (3) Enhancement of NOs' performance by designing proper orthogonal bases that align with the operator and domain, (4) Improvement of NOs' through the allocation of suitable grids, and (5) Investigation of super-resolution error. Our theory has multiple implications in practice: choosing the orthogonal basis and grid points to accelerate training, improving the generalization and super-resolution capabilities, and adapting NO to irregular domains. Corresponding experiments are conducted to verify our theory. Our paper provides a new perspective for studying NOs.

## 1 Introduction

Partial differential equation (PDE) operators are widespread in science and engineering. However, traditional numerical methods are known to be slow and ill-suited for high-dimensional problems. As a result, there has been a surge in the popularity of utilizing deep learning techniques for operator learning. Neural operators (NOs) [20; 19; 8; 21] are among the most important models. As a mapping between infinite-dimensional function spaces, each layer of NO contains a kernel operator and a linear transform to convert the input function, followed by nonlinear activation, conducted numerically based on the discretization of the input function on a grid. With appropriate kernels, e.g., shift-invariant kernels in Fourier NO (FNO) [20], facilitates the construction of complex operators. Stacking multiple NO layers further enhances the operator's complexity and demonstrates its universal approximation capabilities [15]. Moreover, empirical evidence reveals that NO exhibits fast convergence and excellent generalization, making it a practical choice. In addition to its operator fitting and generalization abilities, NO can also perform super-resolution tasks. This involves training the model on a low-resolution grid and accurately predicting outcomes on a high-resolution grid. This capability expands the utility of NO beyond precise operator fitting and generalization, showcasing its versatility and accuracy in super-resolution applications.

Insufficient understanding surrounds the design of kernel operators, choice of grid points, and NOs' capabilities, despite their notable features in generalization and super-resolution tasks. Forinstance, while the Fourier basis-based FNO has been established, alternative approaches employing polynomial basis [21] and wavelet basis [8; 28] have been proposed to enhance NO's performance in handling operators related to non-periodic functions and multiscale functions, respectively. However, the underlying reasons for their effectiveness through basis changes remain elusive. Besides, the effectiveness of the super-resolution effect in NO is currently based on empirical observations, and the factors influencing its efficacy remain unknown. Furthermore, NO is commonly trained on a predefined uniform grid with a predetermined sparsity level. The impact of utilizing grids with varying sparsity levels or randomly sampled grid points on NO's performance remains unexplored.

On the other hand, the applicability of NOs to general problems on irregular domains poses a significant challenge, mainly because the kernel operators utilized in popular models [20; 21; 28] are typically defined on regular bounded domains. Extending NO, based on orthogonal bases, to encompass general irregular domains remains a formidable task. The difficulty lies in effectively incorporating the physical information of the domain into the design of NO's basis and grid.

In this paper, we provide a novel perspective for studying NOs by examining the role of orthogonal bases within their kernel operators. The kernel operators in NOs are constructed such that their eigenfunctions are predefined orthogonal bases, and eigenvalues are trainable parameters. This unified view enables the analysis of NOs in various aspects. Firstly, we establish generalization bounds for NOs, considering them mappings between infinite-dimensional function spaces. Moreover, by carefully designing orthogonal bases for the input domain and functions, NOs can be constructed on irregular domains, improving generalization. The impact of grid points on NO convergence and generalization is also investigated. Additionally, we analyze factors influencing the super-resolution error in NOs. Our theory carries practical implications, such as selecting appropriate orthogonal bases and grid points to accelerate convergence, enhance generalization and super-resolution abilities, and adapt NOs to irregular domains. Extensive experiments are conducted to validate our theory, which sheds new light on understanding NOs and improving their properties in practical applications.

## 2 Preliminary

### Notation & Problem Definition

**Notation**. We use \(\|\cdot\|_{2}\) to denote vector 2-norm or matrix spectral norm, while \(\|\cdot\|_{l^{2}}\) and \(\|\cdot\|_{L^{2}}\) are the norms in \(l^{2}\) and \(L^{2}\) spaces, respectively. We use \(|\cdot|\) to denote the cardinality of a set. For a metric space \((S,\rho)\) and \(T\subset S\) we say that \(\hat{T}\subset S\) is an \(\epsilon\)-cover of \(T\), if \(\forall t\in T\), there \(\exists\hat{t}\in\hat{T}\) such that \(\rho(t,\hat{t})\leq\epsilon\). The \(\epsilon\)-covering number of \(T\) is defined as [13; 29]: \(\mathcal{N}(\epsilon,T,\rho)=\min\{|\hat{T}|:\hat{T}\text{ is an }\epsilon-\text{cover of }T\}\).

**Problem Definition**. We consider the operator learning problem in [20] on the (possibly irregular or unbounded) domain \(\Omega\subset\mathbb{R}^{d}\), with the input function space \(\mathcal{A}=\mathcal{A}(\Omega)\) and output function space \(\mathcal{H}=\mathcal{H}(\Omega)\), and the operator to be learned \(\mathcal{G}:\mathcal{A}\rightarrow\mathcal{H}\). Adding a project can extend the model to different input and output domains. We are given training data \(\{f_{j},\mathcal{G}(f_{j})\}_{j=1}^{N_{\text{min}}}\), where \(f_{j}\sim\mu\) are \(i.i.d.\) samples from an unknown distribution \(\mu\) over the functions supported on \(\mathcal{A}\). We aim to approximate \(\mathcal{G}\) by a neural operator (NO) \(\mathcal{G}_{\theta}\), which requires discretization. Thus, the input functions are represented by their pointwise values on a discrete grid \(\{\vec{\mathbf{x}},f_{j}(\vec{\mathbf{x}})\}=\{\mathbf{x}_{i},f_{j}(\mathbf{x}_{i})\}_{i=1}^{N _{\text{grid}}}\). Labels are also discretized on the same grid \(\{\vec{\mathbf{x}},\mathcal{G}(f_{j}(\vec{\mathbf{x}}))\}=\{\mathbf{x}_{i},\mathcal{G}(f_ {j})(\mathbf{x}_{i})\}_{i=1}^{N_{\text{grid}}}\), and train \(\mathcal{G}_{\theta}\) via minimizing \(\mathcal{L}_{\text{train}}(\theta)=\frac{1}{N_{\text{min}}}\sum_{j=1}^{N_{\text {min}}}\frac{1}{N_{\text{grid}}}\left\|\mathcal{G}(f_{j})(\vec{\mathbf{x}})- \mathcal{G}_{\theta}(f_{j})(\vec{\mathbf{x}})\right\|_{2}^{2}\). The corresponding (regular) test loss on the same grid \(\vec{\mathbf{x}}\) is \(\mathcal{L}_{\text{test-reg}}(\theta)=\mathbb{E}_{f\sim\mu}\frac{1}{N_{\text {grid}}}\left\|\mathcal{G}(f)(\vec{\mathbf{x}})-\mathcal{G}_{\theta}(f)(\vec{\mathbf{ x}})\right\|_{2}^{2}\). We also consider the super-resolution task, i.e., the model can make predictions on all \(\mathbf{x}\in\Omega\), which is approximated by taking another different and usually finer grid \(\vec{\mathbf{x}}_{\text{test}}\) as input and yields the output function pointwise values on the new grid. Note that the characteristic of NO is to take in an arbitrary grid and output the values of the target function on this grid, and the grid size can be arbitrary [19; 20]. This reflects that NO is a mapping between infinite-dimensional spaces. The super-resolution test loss on a different grid \(\vec{\mathbf{x}}_{\text{test}}\) is \(\mathcal{L}_{\text{test-sr}}(\theta)=\mathbb{E}_{f\sim\mu}\left\|\mathcal{G} (f)-\mathcal{G}_{\theta}(f)\right\|_{L^{2}(\Omega)}^{2}\approx\mathbb{E}_{f \sim\mu}\frac{1}{N_{\text{grid}}}\left\|\mathcal{G}(f)(\vec{\mathbf{x}}_{\text{ test}})-\mathcal{G}_{\theta}(f)(\vec{\mathbf{x}}_{\text{test}})\right\|_{2}^{2}\).

### Understanding Neural Operator

Given an input function \(f:\Omega\subset\mathbb{R}^{d}\rightarrow\mathbb{R}^{h}\) where \(h\) is the hidden/output dim, and the complete orthogonal basis set \(\{\phi_{i}\}_{i=0}^{\infty}\) on \(L^{2}(\Omega;\mathbb{R})\) arranged with increasing frequencies/degrees/orders, each layer of NOs contains three operations: (1) the kernel transform, (2) the linear transform, and (3) the nonlinear activation. The overall model structure can be summarized as follows.

**Overall Model**. Denote the input function as \(\hat{u}_{0}(\mathbf{x})\), then the recursive formulation of NO is

\[\hat{u}_{l}(\mathbf{x}) =\sigma\left(u_{l}(\mathbf{x})\right),\;l\geq 1;\;\hat{u}_{0}(\mathbf{x})= \hat{u}_{0}(\mathbf{x}),\] \[u_{l+1}(\mathbf{x}) =\int K(\mathbf{B}_{l},\mathbf{x},\mathbf{y})\hat{u}_{l}(\mathbf{y})d\mathbf{y}+\mathbf{W} _{l}\hat{u}_{l}(\mathbf{x}), \tag{1}\] \[v(x) =u_{L}(x),\]

where \(v(x)\) is the output, \(l\) is the layer index, and \(L\) is the total number of layers. \(\hat{u}\) and \(u\) are the post-activation and pre-activation input functions, respectively.

**Kernel Transform**\(\kappa\). We set up trainable kernels such that the basis set with \(N_{\text{modes}}\) lowest frequencies form the eigenfunctions of the kernels, i.e., \(K(\mathbf{B};\mathbf{x},\mathbf{y})=\sum_{i=0}^{N_{\text{modes}}}\mathbf{B}_{i}\phi_{i}(\mathbf{x} )\phi_{i}(\mathbf{y})\) where \(\mathbf{B}_{i}\in\mathbb{R}^{h\times h}\) and \(\mathbf{B}\in\mathbb{R}^{N_{\text{modes}}\times h\times h}\) are trainable, and \(h\) is the hidden dim. Thanks to the low-rank property of the kernel, i.e., the dimension of its kernel spectra are only \(N_{\text{modes}}\), \(\kappa(f)(\mathbf{x})=\int K(\mathbf{B};\mathbf{x},\mathbf{y})f(\mathbf{y})d\mathbf{y}=\sum_{i=0}^{N_ {\text{modes}}}\mathbf{B}_{i}\mathbf{c}_{i}\phi_{i}(\mathbf{x})\in\mathbb{R}^{h}\), where \(\mathbf{c}_{i}=\langle f,\phi_{i}\rangle\in\mathbb{R}^{h}\) is the dimension-wise inner product between the input \(f\) and the basis \(\phi_{i}\). This form is exactly the same as the kernel in FNO if the basis \(\phi_{i}\) is Fourier series. For implementation, we usually (1) project the input function onto the basis with \(N_{\text{modes}}\) lowest frequencies to get \(\{\mathbf{c}_{i}\}_{i=1}^{N_{\text{modes}}}\); (2) linear transform the coefficients \(\mathbf{c}_{i}\) with the matrices \(\mathbf{B}_{i}\); (3) finally project from the coefficient space back to the function space by multiplying \(\phi_{i}(\mathbf{x})\) to each \(\mathbf{B}_{i}\mathbf{c}_{i}\).

**Linear Transform**\(\omega\). The linear transform \(\omega\) is a straightforward operation \(\omega[f](\mathbf{x})=\mathbf{W}f(\mathbf{x}),\mathbf{W}\in\mathbb{R}^{h\times h},f(\mathbf{x})\in \mathbb{R}^{h}\). Unlike the kernel operator, the linear transform can be conducted in the original function space without projection. The final output is obtained by combining the results from the kernel transform and the linear transform: \(T[f](\mathbf{x})=\kappa(f)(\mathbf{x})+\omega[f](\mathbf{x})\). Finally, the transformer function is passed through a nonlinear activation, i.e., \(\sigma\left(T[f](\mathbf{x})\right)\).

**Examples of Bases**. In FNO [20], the orthogonal basis used is the Fourier basis, specifically over a bounded regular domain. However, alternative orthogonal bases are employed in other variations of FNO, such as those discussed in [8; 21], including polynomial and wavelet bases. Regarding the implementation of the most popular FNO [20]: (1) is the fast Fourier transform (FFT); (2) is its coefficient transform; (3) is the inverse FFT.

**Numerical Integration**. In practice, input functions are discretized on a grid. Transformations and activations can be applied pointwise, but kernel transforms relying on the inner product with bases require numerical integration on the grid. However, the linear transform does not need numerical integration and can be implemented more easily in the original function space.

**Efficiency of Numerical Integration**. In FNO, the Fast Fourier Transform (FFT) is used to compute integrals, with a time complexity of \(O(N_{\text{grid}}\log N_{\text{grid}})\). However, in practice, only the first \(N_{\text{modes}}\) integrations between the basis and input need to be calculated. This reduces the complexity to \(O(N_{\text{grid}})\) since \(N_{\text{modes}}\ll N_{\text{grid}}\). This approach, adopted in Geo-FNO [19], ensures that the integration step does not become a bottleneck in the NO model's time complexity. As a result, NO models are generally faster than Transformers.

## 3 Theory

We have examined the traditional function perspective of NOs. However, machine learning often overlooks the complexity of mappings between infinite-dimensional functions. To address this, we propose studying NOs in the coefficient space. By representing NOs as mappings between infinite sequences of real numbers, derived from the expansion of input functions on an orthogonal basis, we can leverage the extensive literature on mappings between finite-dimensional vectors and extend it to our context. This enables a comprehensive analysis of NOs from a new perspective.

Given a complete orthogonal basis \(\{\phi_{i}\}_{i=0}^{\infty}\), the input function \(f\) and the output function \(\mathcal{G}(f)\) can be expanded as \(f=\sum_{i=0}^{\infty}c_{i}\phi_{i},\mathcal{G}(f)=\sum_{i=0}^{\infty}d_{i}\phi _{i}\) where \(c_{i}=\langle f,\phi_{i}\rangle\) and \(d_{i}=\langle\mathcal{G}(f),\phi_{i}\rangle\). For the infinite sums to converge, the infinite sequences \(\{c_{i}\},\{d_{i}\}\in l^{2}\). So, the operator learning problem on \(\mathcal{G}\) can be abstracted to a mapping between infinite sequences between numbers in the \(l^{2}\) space, i.e.,NOs aim to learn the mapping \(\{c_{i}\}_{i=0}^{\infty}\mapsto\{d_{i}\}_{i=0}^{\infty}\). Thus, we can define the input space from the viewpoint of coefficients \(\mathcal{B}:=\{(\langle f,\phi_{i}\rangle)_{i=1}^{\infty},f\in\mathcal{A}\}\). where \(\mathcal{A}\) is the space of input functions.

We reinterpreted the operations in NOs from the sequences mapping perspective in \(l^{2}\).

**Kernel Transform**. It maps \(f=\sum_{i=0}^{\infty}c_{i}\phi_{i}\) to \(\kappa(f)=\int K(\mathbf{B};x,y)f(y)dy=\sum_{i=0}^{N_{\text{modes}}}\mathbf{B}_{i}c_{i} \phi_{i}\), which can be abstracted to: \(c_{\leq N_{\text{modes}}}\mapsto\mathbf{B}_{\leq N_{\text{modes}}}c_{\leq N_{ \text{modes}}},c_{>N_{\text{modes}}}\mapsto 0\), due to the kernel's local rankness and truncation at the \(N_{\text{modes}}\)-lowest frequency.

**Linear Transform**. For an input function \(f(x)\), the linear transform maps \(f(x)\) to \(\mathbf{W}f(x)\). Also, considering the channel-wise operation, from the sequence space point of view, it is \(c_{i}\mapsto\mathbf{W}c_{i}\). Compared to the previous kernel transform, the linear coefficient is the same for all elements in the sequence, but those are different and truncated in the previous kernel transform.

**Nonlinear Activation**. The nonlinear activation \(\sigma\) is an abstract and fixed mapping between \(l^{2}\) to itself, denoted \(\Sigma:l^{2}\to l^{2},c\mapsto\Sigma c\). In the original function space, the mapping is from the input function \(f(x)\) to \(\sigma(f)(x)\) where \(\sigma:C(\Omega)\to C(\Omega)\). However, in the sequence space, the original \(c_{i}\) is \(c_{i}=\langle f,\phi_{i}\rangle\), and \((\Sigma c)_{i}=\langle\sigma(f),\phi_{i}\rangle\). The \(i\)th entry of \(\Sigma c\) may depend on all \(c_{i}\) for every \(i\) since the activation is performed on the whole input function. For instance, for the input function \(f(x)=\cos(kx)\) and the cosine basis, only \(c_{k}=0\) while other entries equal zero. But \(\sigma(f)(x)=\sigma(\cos(kx))\) is a very complicated function even for simple \(\sigma\) like ReLU, Sigmoid, and Tanh activations, with the post-transformed coefficients \((\Sigma c)_{i}\neq 0\) for all \(i\). Although it is abstract, the Lipschitz continuity is kept:

**Proposition 3.1**.: _If \(\sigma\) is \(L\)-Lipschitz, i.e., \(|\sigma(x)-\sigma(y)|\leq L|x-y|\), then the mapping \(\Sigma\) is also \(L\)-Lipschitz in the \(l^{2}\) space._

**Model Summary**. Denote the input function as \(\hat{u}_{0}(\mathbf{x})\), and its expansion over the orthogonal basis to be \(\{\hat{c}_{0,i}\}_{i=1}^{\infty}\), i.e., \(\hat{u}_{0}=\sum\hat{c}_{0,i}\phi_{i}\), then the recursive formulation of NO in the coefficient space is

\[\begin{split}&\hat{c}_{l,i}=\Sigma c_{l,i},\quad l\geq 1;\\ & c_{l+1,\leq N_{\text{modes}}}=(\mathbf{B}_{l,\leq N_{\text{modes}}}+ \mathbf{W}_{l})\hat{c}_{l,\leq N_{\text{modes}}},\quad c_{l+1,>N_{\text{modes}}}= \mathbf{W}_{l}\hat{c}_{l,>N_{\text{modes}}};\\ & v_{i}=c_{L,i};\end{split} \tag{2}\]

where \(v_{i}\) is the coefficient for the output function, i.e., the output function \(v(\mathbf{x})=\sum_{i=1}^{\infty}v_{i}\phi_{i}(\mathbf{x})\), \(l\) is the layer index, and \(L\) is the total number of layers. For all the indices \(c_{l,i}\), the first \(l\) is for the layer, while the second \(i\) is for the index of the orthogonal basis. \(\hat{c}\) and \(c\) are the post-activation and pre-activation input coefficients, respectively.

### Generalization of NOs

From the sequence perspective, we can derive the generalization bound of NOs via the robustness bound [29, 13]. The generalization gap of the model given in equation (2) can be bounded as follows.

**Theorem 3.1**.: _(Generalization bound of NOs) For any \(\delta\in(0,1)\), with probability at least \(1-\delta\) over the choice of random samples \(S=\{f_{j}\}_{j=1}^{N_{\text{modes}}}\sim\mu\), let the model parameters after optimization to be \(\theta_{S}=\{\{\mathbf{B}_{i,i}\}_{i=1}^{N_{\text{modes}}},\mathbf{W}_{l}\}_{l=0}^{L}\), the following bound holds:_

\[|L_{\text{test-reg}}(\theta_{S})-L_{\text{train}}(\theta_{S})|\leq\prod_{l=0}^ {L}\left(\max_{i}\{\|\mathbf{B}_{l,i}+\mathbf{W}_{l}\|_{2},\|\mathbf{W}_{l}\|_{2}\}\right) \gamma+M\sqrt{\frac{2K\log 2+2\log(1/\delta)}{N_{\text{train}}}}, \tag{3}\]

_for all \(\gamma>0\), where \(K=\mathcal{N}\left(\gamma/2,\mathcal{B},\|\cdot\|_{l^{2}}\right)\) is the \(\gamma/2\)-covering number of the input space \(\mathcal{B}\) under the norm \(\|\cdot\|_{l^{2}}\). \(M\) is the upper bound of the loss function._

All proofs are presented in the Appendix. The generalization bounds, similar to vanilla neural nets, rely on the products of multilayer parameter norms [2, 29]. Theorem 3.1 offers a more detailed characterization of the generalization bounds compared to the findings in [14], and it guides selecting orthogonal bases in NO. We will delve into this topic further in Section 4.

**Extension to Discretized NOs**. In Theorem 3.1, we primarily focus on continuous NOs. However, the presented theory can be extended to discrete NOs by substituting the infinite-dimensional \(l^{2}\) space with a finite-dimensional vector space. In this context, inner products and orthogonal bases can still be defined. The modification lies in the term involving the covering number in the bound \(\mathcal{N}(\gamma/2,\mathcal{B},\|\cdot\|_{l^{2}})\). Here, we replace the \(l^{2}\) space and norm with the finite-dimensional Euclidean space corresponding to the discrete NO and its vector 2-norm.

### Super-resolution Error

Super-resolution involves training a model on a low-resolution grid and evaluating it on a high-resolution grid, with the expectation of comparable performance. While FNO [20] demonstrates excellent super-resolution capabilities, the underlying reasons remain poorly understood. This understanding is crucial for two reasons: (1) enabling training on sparse grids, leading to reduced training time, and (2) ensuring NOs can effectively handle inputs of the same function on different grids and produce satisfactory results.

Before delving into the analysis, it is important to address the numerical integration errors that arise when training NOs on low-resolution grids compared to high-resolution grids during super-resolution. Intuitively, if the integration error is significant, there will be notable discrepancies in the integral values obtained from the sparse training grid and the high-resolution testing grid, resulting in inconsistent model performance between training and testing.

As \(u_{l}\), \(\hat{u}_{l}\), and \(v\) represent variables in continuous NOs, we use \(U_{l}\), \(\hat{U}_{l}\), and \(V\) to represent variables in the discrete NOs using numerical integration rule \(\hat{\int}g\approx\sum_{i=1}^{N_{\text{grid}}}w_{i}g(\mathbf{x}_{i})\) for any integrand \(g\):

\[\hat{U}_{l}(\mathbf{x}) =\sigma\left(U_{l}(\mathbf{x})\right),\;l\geq 1;\quad V=U_{L};\quad U_{0 }=f;\] \[U_{l+1}(\mathbf{x}) =\hat{\int}K(\mathbf{B}_{l},\mathbf{x},\mathbf{y})\hat{U}_{l}(\mathbf{y})d\mathbf{y}+ \mathbf{W}_{l}\hat{U}_{l}(\mathbf{x})=\sum_{i=1}^{N_{\text{grid}}}w_{i}K(\mathbf{B}_{l}, \mathbf{x},\mathbf{x}_{i})\hat{U}_{l}(\mathbf{x}_{i})+\mathbf{W}_{l}\hat{U}_{l}(\mathbf{x}),\]

where \(f\) is the input function, \(w_{i}\) is the weight for numerical integral at the grid point \(\mathbf{x}_{i}\). The error in numerical integration generally depends on two factors: the grid size (defined as \(e_{\text{grid}}(N_{\text{grid}})\)) and the smoothness of the integrand function (defined as \(e_{\text{func}}(f)\)).

For instance, on a uniform grid over the interval \([a,b]\), the integral is approximated using the Darboux method \(\int_{a}^{b}f\approx 1/N_{\text{grid}}\sum_{i=1}^{N_{\text{grid}}}f(x_{i})\) where \(x_{i}=a+(i-1)(b-a)/N_{\text{grid}}\), which yields an integration error of \(O(f^{\prime\prime}(\xi)/N_{\text{grid}})\) where \(\xi\in(a,b)\), i.e., \(e_{\text{grid}}(N_{\text{grid}})=1/N_{\text{grid}}^{2},e_{\text{func}}(f)=f^{ \prime\prime}(\xi)\). However, using the trapezoidal rule instead, the error can be reduced to \(O(f^{\prime\prime}(\xi)/N_{\text{grid}}^{2})\) without requiring additional computations. FNO [20] assumes that the input function is periodic, so the error of the uniform grid decreases to \(O(f^{\prime\prime}(\xi)/N_{\text{grid}}^{2})\). For the Gaussian quadrature on the interval \([a,b]\), the error can be reduced to \(O\left((N_{\text{grid}}!)^{4}f^{2(N_{\text{grid}})}(\xi)/[(2N_{\text{grid}})! ]^{3}\right)\) for \(\xi\in(a,b)\)[10]. With the background, we can write the discretization error of NOs:

**Theorem 3.2**.: _(Discretization error of NOs) Suppose the numerical integration's error is \(e_{\text{grid}}(N_{\text{grid}})e_{\text{func}}(f)\) where \(N_{\text{grid}}\) is the grid size, and \(f\) is the integrand, then the discretization error of discrete NOs compared with continuous ones due to numerical integral is upper bounded by_

\[\|v-V\|_{L^{2}}\leq\sum_{l=0}^{L}\prod_{k=1}^{L}\max_{i}\left\{\|\mathbf{B}_{k,i}+ \mathbf{W}_{k}\|_{2},\|\mathbf{W}_{k}\|_{2}\right\}\left(\sum_{i=0}^{N_{\text{func}}} \|\mathbf{B}_{i,l}\|_{2}e_{\text{grid}}(N_{\text{grid}})e_{\text{func}}\left( \hat{U}_{l}\cdot\phi_{i}\right)\right). \tag{4}\]

The discretization error in discrete NOs relies on the norm of model parameters and the accuracy of the integration method employed for integrating intermediate output functions. Building upon this, we can derive the super-resolution error of discrete NOs. Firstly, we bound the prediction error of continuous NOs across all points in the domain \(\Omega\) (i.e., the super-resolution error of continuous NOs). During the training phase, NOs are trained on a finite training grid, leading to this error. Subsequently, we bound the discrepancy between continuous and discrete NOs, corresponding to the discretization error stated in Theorem 3.2. These two terms are reflected in the following theorem.

**Theorem 3.3**.: _(Super-resolution error of NOs) Assuming a uniform grid on a bounded regular domain with \(N_{\text{grid}}\) points in the FNO [20] setting, under the same notation as Theorem 3.1. Then, the super-resolution error of the discrete NO model for the input function \(f\) with intermediate output \(\hat{U}_{l}\) (as detailed in equation (3.2)) can be bounded as follows:_

\[\begin{split}|L_{\text{total},w}(\theta_{S})&-L_{ \text{total-reg}}(\theta_{S})|\leq\sum_{l=0}^{L}\prod_{k=l}^{L}\max_{i}\left\{\| \mathbf{B}_{k,i}+\mathbf{W}_{k}\|_{2},\|\mathbf{W}_{k}\|_{2}\right\}\left(\sum_{i=0}^{N_{ \text{under}}}\|\mathbf{B}_{l,i}\|_{2}e_{\text{grid}}(N_{\text{grid}})e_{\text{func }}\left(\hat{U}_{l}\cdot\phi_{i}\right)\right)\\ &+\left\{\sum_{i=0}^{N_{\text{under}}}\sum_{l=0}^{L-1}\left(\prod_ {k=l+1}^{L-1}\|\mathbf{W}_{k}\|_{2}\right)\|\mathbf{B}_{l,i}(\hat{u}_{i},\phi_{i})\|_{ 2}\text{Lip}(\phi_{i})+\prod_{l=0}^{L-1}\|\mathbf{W}_{l}\|_{2}\text{Lip}(f)\right\} /N_{\text{grid}}.\end{split} \tag{5}\]The first term pertains to the integral error in Theorem 3.2, while the second term relates to the interpolation and generalization ability of NOs across the entire domain. These factors significantly influence the super-resolution of NOs. Understanding and addressing these factors is crucial for enhancing super-resolution accuracy in NOs, which will be thoroughly discussed in Section 4.

## 4 Implication and Application of the Theory

In this subsection, we introduce the implications and applications of the proposed theory and correspond them to the following numerical experiments.

**Tighter Bound in Theorem 3.1**. Our bound's proof and form are much more general and tighter than previous work [14]. In particular, in terms of parameter matrix norm contributed by each layer, the bound in [14] depends on \(\|\mathbf{W}_{l}\|_{F}+\|\mathbf{B}_{l}\|_{F}N_{\text{modes}}^{d/2}\) where \(\|\cdot\|\) denotes the Frobienus norm. So, our reliance of \(\max_{i}\left\{\|\mathbf{B}_{l,i}+\mathbf{W}_{l}\|_{2},\|\mathbf{W}_{l}\|_{2}\right\}\) is a significant improvement. Additionally, our bound suggests that increasing the number of modes does not necessarily increase the complexity of the model. However, it is important to note that selecting high-frequency basis functions to fit high-frequency noise can adversely impact generalization, as it leads to larger values of \(\|\mathbf{B}_{l,i}+\mathbf{W}_{l}\|_{2}\). We shall verify the advantage of our bound in Experiment 6.1.

**Super-Resolution Error**. From Theorem 3.3, super-resolution in NOs depends on two critical factors: (1) the accuracy of integration trained on low-resolution grids, and (2) the density of the low-resolution grid to facilitate generalization to other points. It is important to note that numerical integration accuracy does not necessarily imply grid density, especially in the case of low-precision integration formats. We shall experiment on the super-resolution error in Experiment 6.2.

**Choice of Grid**. The choice of grids depends on the specific function and its domain. In the case of a finite interval, a uniform grid is commonly used, and integration can be performed using the trapezoidal rule. Furthermore, selecting a grid that allows for accurate integration effectively captures the function's characteristics at a finite number of points. For instance, in density functional theory (DFT), where the domain is \(\mathbb{R}^{3}\), the function typically involves a multi-center Gaussian mixture. In such cases, designing an integral scheme tailored to multi-center functions is more reasonable. For example, selecting points in the vicinity of each Gaussian center enables better characterization and integration accuracy for the input function. We will conduct the corresponding DFT experiment on molecules in 6.3.

**Extending NOs to Irregular Domains**. The limitation of FNO [19; 23] is its reliance on Fourier bases, which restricts its application to regular domains and limits its usage in real-world complex geometries. Geo-FNO [19] assumes that irregular domains can be mapped to regular ones through a bijection, which is not applicable for arbitrarily irregular domains. Fortunately, under our framework in equation (2), we can overcome this limitation by utilizing random Fourier features (RFFs) and polynomials on irregular domains. By employing Gram-Schmidt orthogonalization, we can obtain an orthogonal basis and perform numerical integration on a discrete grid for the inner product. Moreover, we can select a suitable orthogonal basis based on the domain, such as Gauss-Hermite polynomials for the unbounded whole space \(\mathbb{R}\). As a result, we successfully extend NOs to irregular domains, enhancing their applicability in various scenarios. In the first setting of Experiment 6.3, we validate our general NOs on unbounded domains.

**Guiding the Choice of Orthogonal Basis**. The choice of basis in NOs significantly impacts their expressiveness, as highlighted by Theorem 3.1. If the target function can be well represented by a finite combination of basis functions, a small number of modes (\(N_{\text{modes}}\)) is sufficient. This leads to fewer parameters and increased model efficiency. Conversely, if an infinite series of basis functions is needed to expand the target function, the model tends to generalize poorly. For example, Fourier bases are suitable for periodic functions, wavelets excel in capturing rapid changes and discontinuities, and polynomials (e.g., orthogonal Legendre polynomials) provide a versatile basis for all functions. These bases have distinct capabilities and cannot efficiently represent each other. Additionally, wavelets are effective in handling multi-scale and multi-physics problems. Combining multiple basis sets can yield superior results by leveraging complementary effects. The selection of the basis is guided by the characteristics of the function and the operator in the dataset. For instance, if the function exhibits periodicity in certain subdomains but not in others, a combination of Fourier and polynomial basis functions can effectively model both parts simultaneously, which is verified in Experiment 6.4.

Related Work

**Orthogonal Basis in NOs**. Various orthogonal bases are adopted for kernel transform in neural operators, e.g., Fourier NO (FNO) [20] and its variant [25; 27; 30] adopt Fourier bases, Geo-FNO [19] utilizes Fourier bases under deformation, [21] uses orthogonal Legendre polynomials, and [28; 8] use multi-wavelets.

**NOs on Arbitrary Domains**. Some operator networks different from NO are grid-free. DeepONet [22] encodes the input function and the grid, respectively, and then combines them by dot product as the operator network output. MIONet [12] extends DeepONet to multiple input functions. Transformer operators [3; 18; 9] directly process the inputs by the attention mechanism [26]. The NO we proposed can also be applied to any domain and incorporates its prior knowledge, so our NO generally outperforms these two approaches.

**Theory of Neural Operators**. DeepONet and FNO are analyzed theoretically in the literature. Theory on DeepONet [16; 7] relies on the discretization over function and the input grid to transform the model into mapping between finite-dimensional vector space. The generalization theory on FNO [14] relies on discretization and proposes Rademacher complexity bounds. [15] proves the universal approximation and errors bound for approximating Darcy type elliptic PDE and the incompressible Navier-Stokes equations. [5; 6; 23] proposes theories for NO's approximation. [4] proves convergence rates for linear operators.

## 6 Experiment

### Validation of Theorem 3.1

This section provides empirical evidence demonstrating the superior tightness and quality of our generalization bound presented in Theorem 3.1 compared to related works such as [14]. It is customary in the literature to compare the numerical values of various generalization bounds as a means to showcase their tightness, e.g., [24; 1; 11; 2]. In Figure 2, we provide numerical values of the generalization bounds for FNO models trained on four distinct datasets (1D Burgers, 2D Darcy Flow, 2D+time Navier-Stokes equation, 3D Navier-Stokes equation) as provided by FNO [20]. Following FNO's experimental setup, we normalize the value of our proposed bound to 1 for clarity. Remarkably, our robustness-based bound outperforms existing bounds by 2-3 orders of magnitude, underscoring its superior tightness and reliability.

### Super-resolution Error

We validate Theorem 3.3 that super-resolution error is affected by both the integration scheme and the grid size. We conduct experiments on the previously mentioned Burgers dataset from FNO[20], where we use the random scheme (error: \(1/\sqrt{N}\), blue), Darboux rule based on a uniform grid (error: \(1/N\), green), and trapezoidal rule based on a uniform grid (error: \(1/N^{2}\), red) as integration schemes in FNO [20]. The grid sizes are chosen to be \(2^{6}\) (first column), \(2^{8}\) (second column), \(2^{10}\) (third column) points. The super-resolution performance is evaluated by the super-resolution gap \(L_{\text{test-reg}}-L_{\text{test-sr}}\) originally defined in Theorem 3.3. Figure 2 illustrates the following findings: (1) increasing the grid size for the same integration scheme improves super-resolution performance, and (2) for the same grid size, integration schemes with higher accuracy yield lower super-resolution error. These results confirm the validity of our theory. It is important to note that the original FNO framework cannot operate on random grids. Our reinterpretation of NOs, utilizing random grids as the integration format and Fourier bases, enabled this capability.

### NOs on Unbounded Domain

In this subsection, we select several Density Functional Theory (DFT) Hamiltonian operators to test our NOs with suitable orthogonal bases with other strong baselines.

**Baselines**. (1) FNO [20]: cannot be adopted on an arbitrary domain. (2) Geo-FNO [19]: uses a bijection to map the irregular domain to a regular one and perform FNO. Here the unbounded domains can be mapped to the regular domain by the bijection \(\tan^{-1}\), which can conduct super-resolution. (3) DeepONet [22] can operate on arbitrary domains but accepts fixed-length discretized input function, which restricts its super-resolution performance. (4) Linear Transformer (LT) [3] and (5) OFormer [18] are all transformers for operator learning, they can handle arbitrary domains and grids.

In Density Functional Theory (DFT), the Hamiltonian operator plays a crucial role in characterizing the ground state energy through its spectra. In this subsection, we evaluate different NOs' performance in learning Hamiltonian operators defined on unbounded domains across various dimensions.

**QHO**. In the quantum harmonic oscillator (QHO), the Hamiltonian operator, given the wave function \(\phi\), is \(\left(\hat{H}_{\text{QHO}}\phi\right)(x)=-\frac{1}{2}\nabla^{2}\phi(x)+\frac{ 1}{2}x^{2}\phi(x),x\in\mathbb{R}\). We use random linear combinations of the 1D Hermite polynomial for data generation: \(\phi_{i}(x)=\frac{1}{\pi^{\frac{1}{4}}2^{\frac{3}{2}}\sqrt{i!}}H_{i}(x)e^{- \frac{x^{2}}{2}},x\in\mathbb{R}\), The grid for training is the points in the Gauss-Hermite quadrature with 32 points, and the super-resolution testing grid is with degree 64. Geo-FNO adopts the grid generated by the \(\tan^{-1}\) transform of uniform grid over \((-\frac{\pi}{2},\frac{\pi}{2})\). The orthogonal basis in our NO is Gauss-Hermite polynomials.

**Molecules**. Following D4FT [17], we consider Hamiltonian operators in real-world 3D molecules, which take the wave function \(\phi(\mathbf{r})\), and the density \(\rho(\mathbf{r})\) as inputs is given by four different terms (kinetic, external potential, Hartree and exchange-correlation):

\[\left(\hat{H}_{\text{KS-DFT}}(\phi,\rho)\right)(\mathbf{r})=-\frac{1}{2}\nabla^{2 }\phi(\mathbf{r})+v_{\text{res}}(\mathbf{r})\phi(\mathbf{r})+\int_{\mathbb{R}^{3}}\frac{ \rho(\mathbf{r}^{\prime})}{|\mathbf{r}-\mathbf{r}^{\prime}|}d\mathbf{r}^{\prime}\phi(\mathbf{r})+v _{\text{sc}}(\mathbf{r})\phi(\mathbf{r}),\mathbf{r}\in\mathbb{R}^{3}. \tag{6}\]

KS-DFT offers natural basis sets that can be linearly combined to generate the training functions. Additionally, grids with varying resolutions, characterized by levels, are provided based on the multi-center Gaussian nature of the functions in DFT. In our experiments, we select level 1 for training and level 2, which has more points, for testing. In Geo-FNO, the grid is generated using the \(\tan^{-1}\) transform of a uniform grid over the domain \((-\frac{\pi}{2},\frac{\pi}{2})^{3}\). To ensure a fair comparison, the

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline  & FNO & Geo-FNO & DeepONet & LT & OFormer & NO-Ours \\ \hline \hline QHO-vanilla & / & 1.22E-2 & 2.63E-3 & 3.04E-3 & 3.31E-3 & **1.50E-3** \\ \hline QHO-superres & / & 1.32E-1 & 1.42E+0 & 7.15E-3 & 7.91E-3 & **2.02E-3** \\ \hline \hline CO\({}_{2}\)-vanilla & / & 4.97E-1 & 5.82E-1 & 2.46E-1 & 2.41E-1 & **2.22E-1** \\ \hline CO\({}_{2}\)-superres & / & 5.44E-1 & 1.44E+0 & 2.51E-1 & 2.47E-1 & **2.23E-1** \\ \hline \hline Water-vanilla & / & 2.41E-1 & 3.22E-1 & 3.03E-1 & 2.86E-1 & **1.52E-1** \\ \hline Water-superres & / & 5.07E-1 & 1.75E+0 & 4.52E-1 & 3.07E-1 & **1.58E-1** \\ \hline \hline CH\({}_{4}\)-vanilla & / & 3.16E-1 & 5.19E-1 & 2.76E-1 & 2.62E-1 & **2.05E-1** \\ \hline CH\({}_{4}\)-superres & / & 3.95E-1 & 1.82E+0 & 2.81E-1 & 2.79E-1 & **2.07E-1** \\ \hline \end{tabular}
\end{table}
Table 1: Relative \(L^{2}\) error results on DFT datasets with unbounded domains. ANO achieves the best result and can conduct SR.

number of grid points in Geo-FNO is similar to the quadrature used in D4FT. For the D4FT setting, we consider three molecules: CO\({}_{2}\), water, and CH\({}_{4}\).

**Results**. Relative \(L^{2}\) errors for DFT experiments are shown in Table 1. (1) NO-Ours exhibits slightly superior performance on regular testing and significantly outperforms in super-resolution tasks. (2) The uniform grid deformation in Geo-FNO is less efficient compared to quadrature, resulting in NO-Ours surpassing Geo-FNO. (3) As the molecule size increases, atom distribution extends throughout the \(\mathbb{R}^{3}\) domain. Consequently, the input wave function and density function disperse near the atoms rather than concentrating near the unit box. Thus, using a unit box grid in Geo-FNO becomes inefficient. (4) NO-Ours adapts to various grids based on the problem, utilizing efficient quadrature points in D4FT to accurately represent the input wave function and density. As a result, NO-Ours achieves super-resolution with minimal additional error. This highlights the significance of selecting appropriate integral points based on the characteristics of the input functions.

### Combining Multiple Bases

We try the advection equation \(u_{t}+u_{x}=0,x\in[0,1],t\in[0,1]\) with/without periodic boundary conditions taken from [23]. Given the initial condition \(u_{0}(x)\), we aim to learn the non-linear operator \(\mathcal{G}:u_{0}(x)\mapsto u(x,t)^{2},(x,t)\in[0,1]\times[0,1]\) with the hybrid input functions \(u_{0}(x)=h_{1}1_{\{c_{1}-\frac{u}{2},c_{1}+\frac{u}{2}\}}+\sqrt{\max(h_{2}^{2 }-a^{2}(x-c_{2})^{2},0)}\) where \(c_{1},c_{2},w,h_{1},h_{2}\) are randomly chosen to generate samples. The full problem is named Advection (1), which is periodic in both \(t\) and \(x\) axis, i.e., \(u(x,0)=u(x,1)\) and \(u(0,t)=u(1,t)\). To construct a non-periodic problem, we truncate the target function on \((x,t)\in[0,1]\times[0,0.5]\), so that it is periodic on the \(x\)-axis but not the \(t\)-axis. We use sinusoidal and/or polynomial bases. Specifically, there are two axes, for NO-Sin/Sin, we use sinusoidal bases on both axes; for NO-Poly/Poly, we use Legendre polynomial bases on both axes; for NO-Sin/Poly, we use sinusoidal on the \(x\)-axis and polynomial on \(t\)-axis. More specifically, if we have a set of basis functions \(\{\phi_{i}(x)\}_{i=0}^{\infty}\) on the domain \(\Omega_{x}\) along the \(x\)-axis, and another set of basis functions \(\{\psi_{j}(t)\}_{i=0}^{\infty}\) on the domain \(\Omega_{t}\) along the \(t\)-axis, then the set of tensor product basis functions \(\{\phi_{i}(x)\psi_{j}(t)\}_{i,j=0}^{\infty}\) forms a basis for the two-dimensional space \(\Omega_{x}\times\Omega_{t}\). We train all models with 1000 epochs.

Table 2 verifies the effectiveness of sinusoidal bases for periodic functions and polynomials for general non-periodic functions. Additionally, the combination of multiple different bases has been shown to be effective, taking into account the properties of the data. This approach deviates from previous works that typically focus on utilizing a single basis.

## 7 Conclusion

This paper proposes a novel perspective for studying NOs. We have provided a comprehensive understanding of NO through a detailed analysis of the infinite sequence space under orthogonal basis projection. Based on this versatile framework, we have proposed a method to adapt NO to arbitrary complex domains. We have also analyzed the generalization bound of NO, demonstrating its superiority over previous works. Furthermore, we have explained the importance of selecting the type and quantity of basis functions in NO, emphasizing the benefits of using multiple bases in a complementary manner based on operator characteristics. Additionally, we have examined the impact of grid points on super-resolution error, highlighting the crucial role of the integration format associated with the grid and the density of the grid itself. All the theoretical analyses have been extensively validated through experiments on multiple data sources, including numerical PDE and DFT. We shed new light on understanding NOs and improving them in practical applications.

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline  & NO-Sin/Sin & NO-Poly/Poly & NO-Sin/Poly \\ \hline Advection (1) & **8.34E-3** & 1.96E-2 & 1.01E-2 \\ \hline Advection (2) & 1.00E-2 & 1.76E-2 & **7.66E-3** \\ \hline \end{tabular}
\end{table}
Table 2: Relative \(L^{2}\) error results for the advection case.

## References

* Arora et al. [2018] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In _International Conference on Machine Learning_, pages 254-263. PMLR, 2018.
* Bartlett et al. [2017] Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. _arXiv preprint arXiv:1706.08498_, 2017.
* Cao [2021] Shuhao Cao. Choose a transformer: Fourier or galerkin. _Advances in Neural Information Processing Systems_, 34:24924-24940, 2021.
* de Hoop et al. [2021] Maarten V de Hoop, Nikola B Kovachki, Nicholas H Nelsen, and Andrew M Stuart. Convergence rates for learning linear operators from noisy data. _arXiv preprint arXiv:2108.12515_, 2021.
* De Ryck and Mishra [2022] Tim De Ryck and Siddhartha Mishra. Generic bounds on the approximation error for physics-informed (and) operator learning. _arXiv preprint arXiv:2205.11393_, 2022.
* Franco et al. [2023] Nicola Rares Franco, Stefania Fresca, Andrea Manzoni, and Paolo Zunino. Approximation bounds for convolutional neural networks in operator learning. _Neural Networks_, 161:129-141, 2023.
* Gopalani et al. [2022] Pulkit Gopalani, Sayar Karmakar, and Anirbit Mukherjee. Capacity bounds for the deeponet method of solving differential equations. _arXiv preprint arXiv:2205.11359_, 2022.
* Gupta et al. [2021] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential equations. _Advances in neural information processing systems_, 34:24048-24062, 2021.
* Hao et al. [2023] Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, and Jian Song. Gnot: A general neural operator transformer for operator learning. _arXiv preprint arXiv:2302.14376_, 2023.
* Bgenaud Hildebrand [1987] Francis Bgenaud Hildebrand. _Introduction to numerical analysis_. Courier Corporation, 1987.
* Hu et al. [2022] Zheyuan Hu, Ameya D Jagtap, George Em Karniadakis, and Kenji Kawaguchi. When do extended physics-informed neural networks (xpinns) improve generalization? _SIAM Journal on Scientific Computing (SISC)_, 2022.
* Jin et al. [2022] Pengzhan Jin, Shuai Meng, and Lu Lu. Mionet: Learning multiple-input operators via tensor product. _SIAM Journal on Scientific Computing_, 44(6):A3490-A3514, 2022.
* Kawaguchi et al. [2022] Kenji Kawaguchi, Zhun Deng, Kyle Luh, and Jiaoyang Huang. Robustness implies generalization via data-dependent generalization bounds. In _International Conference on Machine Learning (ICML)_, 2022.
* Kim and Kang [2022] Taeyoung Kim and Myungjoo Kang. Bounding the rademacher complexity of fourier neural operator. _arXiv preprint arXiv:2209.05150_, 2022.
* Kovachki et al. [2021] Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds for fourier neural operators. _The Journal of Machine Learning Research_, 22(1):13237-13312, 2021.
* Lanthaler et al. [2022] Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A deep learning framework in infinite dimensions. _Transactions of Mathematics and Its Applications_, 6(1):tmac001, 2022.
* Li et al. [2023] Tianbo Li, Min Lin, Zheyuan Hu, Kunhao Zheng, Giovanni Vignale, Kenji Kawaguchi, A.H. Castro Neto, Kostya S. Novoselov, and Shuicheng YAN. D4FT: A deep learning approach to kohn-sham density functional theory. In _The Eleventh International Conference on Learning Representations_, 2023.
* Li et al. [2022] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. _arXiv preprint arXiv:2205.13671_, 2022.

* [19] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned deformations for pdes on general geometries. _arXiv preprint arXiv:2207.05209_, 2022.
* [20] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _International Conference on Learning Representations (ICLR)_, 2021.
* [21] Ziyuan Liu, Haifeng Wang, Kaijuna Bao, Xu Qian, Hong Zhang, and Songhe Song. Render unto numerics: Orthogonal polynomial neural operator for pdes with non-periodic boundary conditions. _arXiv preprint arXiv:2206.12698_, 2022.
* [22] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. _Nature Machine Intelligence_, 3(3):218-229, 2021.
* [23] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. _Computer Methods in Applied Mechanics and Engineering_, 393:114778, 2022.
* [24] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. _arXiv preprint arXiv:1706.08947_, 2017.
* [25] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators. In _International Conference on Learning Representations_, 2023.
* [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [27] Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-fno--an enhanced fourier neural operator-based deep-learning model for multiphase flow. _Advances in Water Resources_, 163:104180, 2022.
* [28] Xiongye Xiao, Defu Cao, Ruochen Yang, Gaurav Gupta, Gengshuo Liu, Chenzhong Yin, Radu Balan, and Paul Bogdan. Coupled multiwavelet operator learning for coupled differential equations. In _The Eleventh International Conference on Learning Representations_, 2023.
* [29] Huan Xu and Shie Mannor. Robustness and generalization. _Machine learning_, 86(3):391-423, 2012.
* [30] Jiawei Zhao, Robert Joseph George, Yifei Zhang, Zongyi Li, and Anima Anandkumar. Incremental fourier neural operator. _arXiv preprint arXiv:2211.15188_, 2022.