# Data Curation for Image Captioning with

Text-to-Image Generative Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent advances in image captioning are driven by increasingly larger-scale vision-language pretraining, relying on massive computational resources and increasingly large datasets. Instead of solely focusing on scaling pretraining, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on the COCO and Flickr30K datasets. Models trained with our data curation approaches consistently outperform their baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the errors made by the Stable Diffusion model and highlight directions for future work in text-to-image generation.

## 1 Introduction

Large-scale vision-language pretraining has been the driving force behind recent advances in image captioning [14]. The amount of image-text data needed to pretrain recent generative language models [28, 23, 53] has made it necessary to train on "noisy" samples harvested from the web [46, 45], as opposed to crowdsourced captions [32]. This emerging reliance on harvested data has made it important to perform additional filtering steps to remove low-quality data [28], in addition to more resource-intensive pretraining. Given that computing resources are not equally distributed [21], there is a need to also pursue less resource-intensive research directions.

We show how to improve image captioning by improving the quality of the downstream task data through _data curation_: the process of dynamically updating the samples during training. We devise three techniques for data curation that are designed to prevent the total size of the dataset from increasing: the complete removal of an image-caption sample from a dataset; replacing a caption with another caption; and replacing images using a text-to-image generation model [41]. These curation techniques are used to update image-caption samples that have outlier losses, with respect to the rest of a training dataset, under the current model parameters. In other words, the samples that are proving _difficult_ to model. Also, the synthesis of completely new images is radically different from standard data augmentation techniques, such as random cropping or color manipulation [47], or swapping and mask words in text [12].

We conduct experiments using BLIP [28], a strong image captioning model, on the Flickr30K [56] and MS COCO datasets [32]. The results show that the sample removal and image replacement techniques lead to consistent improvements of 1-3 CIDEr points compared to not curating the dataset. Our analyses show that Flickr30K benefits from more curation than COCO due to differencesin the distribution of long captions in each dataset. Finally, we find that it is better to curate the data dynamically while training instead of replacing images before starting to train the model. Taken together, these findings show the promise of _model-in-the-loop_ text-to-image generation for multimodal learning, while highlighting that improvements in text-to-image generation are likely to further enhance the effectiveness of data curation.

## 2 Related work

Image CaptioningImage Captioning is the task of describing images with syntactically and semantically sentences. Current deep learning-based image captioning models have evolved as the encode-decoder frameworks with multi-modal connection [8; 9], attentive [24; 16] and fusion strategies [58]. Standard captioning datasets contain Flickr30K [56] and the commonly used MS COCO [32], which consisting of images with events, objects and scenes. Each image is paired with five captions. Some works have demonstrated the benefits of training on synthetic captions [29; 3] or datasets collected from other vision-and-language learning tasks [38; 7].

Data AugmentationData augmentation [13] has achieved increasing attention in both natural language processing [33] and vision-and-language learning [27]. Early methods generate augmented examples in the model's feature space [54] or interpolate the inputs and labels of few examples [57]. For downstream tasks in the text domain, Yang et al. [55] and Anaby-Tavor et al. [1] generate synthetic text examples through state-of-the-art pretrained language models and show improved performance on common-sense reasoning and text-classification. For image captioning, BERT [11] has been used to generate additional captions to improve the diversity of the captioning datasets [3]. Hossain et al. [22] used GAN-synthesized images as additional augmentation training set to improve image captioning models.

Diffusion Models and ApplicationDiffusion models [49; 35] have grown rapidly and become the powerful deep generative models. They have shown potential in a variety of applications, including text-to-image generation [36; 15], image-to-image translation [42], as well as semantic segmentation [26; 5] and video generation [20; 48; 52]. While recent large scale latent diffusion models have shown strong capability in generating both artistic and photo-realistic high-resolution images [41; 34; 39; 43], applying large-scale stable diffusion models in vision-language downstream tasks remains under-explored. Concurrently, Azizi et al. [4] and Jain et al. [25] show that image classifiers can be improved by learning from augmentation images generated by finetuned stable-diffusion models. To the best of our knowledge, we are the first to explore how image captioning models can benefit from simple data curation without scaling up existing datasets, and how stable-diffusion text-to-image models can be applied and contribute in the process.

Figure 1: Overview of our data curation approaches. For dynamic removal or replacement of captions, high loss image-text pairs are either removed or the image is paired with an alternative caption in the following training epoch. For image replacement, captions of original images are used as prompts for text-to-image generation to synthesize new image–text pairs. We experiment with both options of replacing the image only, or pair another relevant caption to the synthesized image.

Data Curation for Captioning

Our goal is to improve image captioning models by preventing the model from training on difficult samples. There are many reasons for the possible existence of these difficult samples, including mismatches or inconsistencies between the image and caption [3]. More formally, given an image captioning training dataset \(\mathcal{D}\) with \(K\) images, let \(\mathbf{l}_{k}\) be the \(k\)-th image. Each image is paired with \(J\) captions; let \(\mathbf{C}_{k}^{j}\) be \(j\)th caption of image \(k\), and thus, let (\(\mathbf{l}_{k}\), \(\mathbf{C}_{k}^{j}\)) be an image-caption sample in the dataset. Assume the existence of model \(\mathcal{M}\), which is being trained on dataset \(\mathcal{D}\), from which we can calculate the loss of each sample at each epoch \(t\): \(\mathcal{L}_{\mathcal{M}}^{t}(\mathbf{l}_{k},\mathbf{C}_{k}^{j})\), which can be used to track the difficult samples. At the end of each epoch, the difficult samples are candidates for our data curation techniques, resulting in dynamic updates to the training dataset \(\mathcal{D}\rightarrow\mathcal{D}_{1}\rightarrow\cdots\rightarrow\mathcal{D} _{T}\).

### Identifying the difficult samples

Difficult training samples may contain mismatches or inconsistencies between the image and the caption [3]. We propose to use the captioning model that is being trained to automatically identify such samples. After each epoch, we compute the loss of each sample in the current training dataset, given the current model parameters. The highest loss samples are targets for our data curation methods; more specifically, we focus on samples with losses that are either two standard deviations from the mean, or a fixed X% away e.g. 10%, 20%, etc. In this way, the training dataset is dynamically updated at the end of each epoch according to the model's captioning capability. The adjacent figure shows the empirical distribution of losses in the training samples of the Flickr30K dataset. It is clear that, without data curation, the high-loss samples remain high-loss during five epochs of training.

### Sample Removal / Caption Replacement

The simplest approach to data curation is to remove or replace the high-loss samples. In Remove, the high-loss samples are completely removed from the remainder of the training process, reducing the total number of image-caption training samples. In ReplaceCap, we simply replace the caption in the image-caption sample with a different caption taken from the other captions that describe the image, effectively creating a duplicate. With the caption replacement method, the total number of samples used to train the model remains the same, as well as the total number of the unique images. This creates a clean control condition for the subsequent experiments.

### Image Generation-based Replacement

An alternative to removing difficult samples or replacing captions is to pair an existing caption with a new image. This has the benefit of training the model on the same total number of samples while exposing it to more unique images. The new image could be found by humans, in a long-running human-in-the-loop cycle. Instead, we use a text-to-image generation model, in a rapid model-in-the-loop step, to synthesize images based on the other sentences that describe the image. Some representative examples of images generated using this technique can be seen in Figure 10.

Our methodology is based on the open source Stable Diffusion model [41], which can generate images given a textual prompt.1 We integrate this into training as follows: Given an image \(I_{k}\) in the training data and its captions \(\{(I_{k},C_{k}^{1}),\ldots,(I_{k},C_{k}^{j})\}\), we synthesize a new image \(\hat{I}_{k}\) without increasing the total number of samples in the original dataset. Instead, we replace the original image in the sample with the generated image. Specifically, for image \(I_{k}\), we replace a high-loss sample \((I_{k},C_{k}^{j})\) with the synthesized image-text pair \((\hat{I}_{k},C_{k}^{j})\).

Footnote 1: It is also possible to use API-based models but we chose Stable Diffusion for two reasons: (i) Stable Diffusion can be integrated directly into our training pipeline using the open source code. And (ii) we estimate that it would cost $7,424 to run a single experiment on the Flickr30K dataset using DALLE-2.

Figure 2: Distribution of per-sample losses in Flickr30K.

### Round-trip captioning evaluation

In order to effectively use a text-to-image generation model for data curation, we need an objective measure that can estimate the expected quality of a generated image. Most previous work uses image-oriented measures like FID [19] or CLIPScore [17] but these measures are claimed to lack alignment with perceptual quality [44]. We also found they were not suitable for our purpose, and that CLIPScore cannot distinguish between low- and high-loss samples in the captioning model (Figure 9). Here, we propose an alternative that is directly related to our task: given the generated image, measure the quality of the caption that can be generated by a fixed model. Our assumption is that if the generated images are of a similar quality to the original images, the resulting captions should be similar to each other. We call this a round-trip captioning evaluation, which comprises three steps illustrated in Figure 3. In Step (1), we use the captions in the validation set to generate images using a text-to-image generation model. In Step (2), we use an existing image-captioning model to predict captions for the generated images. Specifically, we use BLIP fine-tuned on the COCO dataset but any other strong captioning model could be used instead. Finally, in Step (3), we compare the predicted captions against the original captions. We now discuss the the factors that we found make a difference when generating images.

### Prompt engineering matters

Recall that text-to-image generation models produce images based on a textual prompts. Given a set of five captions that describe an image, there are several options for how to prompt the image generation model. We experiment with three options:

* Single caption: Each caption is used in isolation to generate a new image.
* Sentence-BERT selection: There is a lot of variety in how different captions describe the same image. Instead of using all captions, we can use a representative caption from the set. This is achieved using the Sentence-BERT [40] model to find the caption that is closest to the average embedding of all captions.
* Concatenation: All five captions are concatenated as the text prompt for generation.

For all three approaches mentioned above, we can append an additional string to the prompt as a _styler_ to force a specific style in the generated image (+Styler). The styler used here is: "national geographic, high quality photography, Canon EOS R3, Flickr".2

Footnote 2: The styler was chosen by inspecting the generated images, with a preference against “artistic” outputs.

### Finetuning improves image relevance

Table 1 shows the results of the round-trip captioning evaluation on the Flickr30K dataset using different textual prompts and whether or not to fine-tune the diffusion model. When we fine-tune StableDiffusion, we use the MS COCO [32] dataset with a prompt consisting of a concatenation of all 5 captions, for 15,000 steps with a constant learning rate of \(1e-5\) and a batch size of 32. The best performance is clearly found by fine-tuning Stable Diffusion 1.5 and using a prompt with a concatenation of the captions and the styler. We use this configuration in the remainder of the paper.

\begin{table}
\begin{tabular}{l l l c c c} \hline \hline Model & FT & Prompt & B & C & M \\ \hline Upper-bound & & & 37.6 & 27.2 & 57.1 \\ SD 1.5 & - & concat & & 31.0 & 24.7 & 52.5 \\ SD 1.5 & - & + styler & 30.8 & 24.2 & 52.5 \\ SD 1.5 & F & + styler & **33.5** & **25.0** & **53.5** \\ SD 1.5 & F & SBERT + styler & 30.6 & 24.1 & 52.0 \\ SD 2.0 & - & concat + styler & 31.2 & 24.8 & 52.0 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Round-trip captioning evaluation on Flickr30K with different Stable Diffusion models, prompts, and fine-tuning. BLEU, CIDEr, Meteor.

Figure 3: Round-trip captioning evaluation.

## 4 Experiments

We evaluate our data curation methods on the MS COCO and Flickr30K datasets when finetuning the pretrained BLIP [28] model. We evaluate the captions using BLEU [37], METEOR [10], **R**OUGE [31], CIDEr [51], SPICE [2], **CLIPScore**, and **RefCLIPScore**[18].

We use the ViT-based BLIP model [28] as our captioning model. We note that BLIP has a captioning and filtering (CapFilt) data augmentation process during its pretraining, where both components were finetuned on the COCO dataset. Therefore we use pretrained checkpoint BLIP\({}_{CapFilt}\) for Flickr30k and BLIP\({}_{base}\) for COCO in our experiment, removing the effects from the CapFilt process. We finetune BLIP using a batch size of \(128\) for \(5\) epochs on \(4\times\) A100 GPUs.

### Results

Removal/Caption ReplacementAs shown in Table 2, dynamically removing mismatched image-text pairs or replacing captions can effectively improve performance on both datasets over baselines on all metrics. For Flickr30K, the dynamic updates work best when apply to the top 1% of high-loss samples for ReplaceCap, and to samples whose loss are two standard deviations higher than the mean for Remove. For COCO, both ReplaceCap and Remove works best when curating the top 1% of high-loss samples. We repeat that during the curation process, no additional data samples or computation cost is introduced. We further study the effect of the amount of curation in Section 5.

Image Generation-based ReplacementWe evaluate Image Generation-based Replacement on both the Flickr30K and COCO dataset. During finetuning, we replace images in the original text-image pairs with Stable Diffusion-synthesized images (ReplaceImg in Table 2). The results show improvements compared to the baseline in every evaluation measure with best performance obtained at replacement ratio of 40% for Flickr30K and at 10% for COCO. We show qualitative examples in Figure 4, where models finetuned with our proposed curation method can generate better captions for some scenes that may confuse the standard finetuned model. In Section 5.1, we analyze the effects of varying the amount of synthetic images replaced, and in Section 5.2, we conduct a human study of the types of errors found in the generated images.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Method & B & M & R & C & S & CS & RCS \\ \hline \hline Flickr30K & & & & & & & \\ BLIP & 37.6 & 27.2 & 57.1 & 92.8 & 20.1 & 78.6 & 81.1 \\ +Remove & 38.6 & **27.4** & **57.5** & **95.8** & 21.0 & **79.2** & 81.9 \\ +ReplaceCap & 37.9 & **27.4** & 57.4 & 94.5 & **21.1** & 78.9 & 81.5 \\ +ReplaceImg & **39.0** & 27.3 & 57.4 & 95.7 & 20.7 & 79.1 & **82.0** \\ COCO & & & & & & & \\ BLIP & 39.9 & 30.8 & 59.9 & 132.0 & 23.8 & 77.3 & 82.8 \\ +Remove & 40.1 & 30.9 & 60.0 & 132.5 & 23.6 & 77.3 & 82.8 \\ +ReplaceCap & **40.2** & 30.9 & **60.1** & 132.7 & **23.9** & 77.3 & 82.8 \\ +ReplaceImg & **40.2** & **31.0** & **60.1** & **133.1** & **23.9** & 77.3 & 82.8 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results for standard finetuning with data curation. We find improvements for all curation methods compared to the baseline of training on the original datasets. Best scores are in **bold**.

Figure 4: Qualitative examples from the COCO dataset of captions generated by the BLIP model (top), and the same model trained using our ReplaceImg data curation (bottom). The errors made by the BLIP model (shown in red) are avoided by ReplaceImg curation (shown in blue).

## 5 Analysis and Discussion

### Data Curation: how much and when?

We analyze how the amount of curation affects image captioning performance. We examine different ratios of training samples that are removed, replaced with an alternative caption, or replaced with a synthesized image. For Remove and ReplaceCap, we consider curation ratio of 1%, 5% and 10% of high-loss samples. For ReplaceImg, we consider 10%-80% curation ratio. In addition to fixed X% ratios, we also intereven on samples that have losses two standard deviations worse than the mean.

Flickr30K needs more curation than COCO.The results of this analysis are shown in Figure 5. The best improvement in performance for Flickr30K is achieved either through removing high loss samples that are two standard deviations away, or replacing images for 40% of the high loss samples. In the COCO dataset, replacing images for 10% of the high loss samples gives the best improvement compared to no data curation. The second best performing method for COCO is removing or replacing captions of only 1% of the high loss samples. This indicates that Flickr30K may contain more noisy samples than the MS COCO dataset. Compared to MS COCO, Flickr30K contains more samples with long captions (Figure 6), which may include overly-specific details that are inconsistent with other captions and are hard for the model to learn. See more examples in our supplemental materials. Through our curation-based finetuning, these samples can be effectively identified, removed or replaced, which indicates that our method is efficient when training with noisy datasets. We note that curating more than 50% of the data does not benefit training and actually harms performance.

Static image replacement versus dynamic replacementIn ReplaceImg (Section 3.3), we dynamically replace images for the difficult training samples. Another static approach is to replace the identical images, i.e. \(I_{k}\) in \(\{(I_{k},C_{k}^{1}),\ldots,(I_{k},C_{k}^{J})\}\), with unique SD-synthesized images before training, instead of updating the training samples while training. With static image replacement, for each of the reference captions, we replace their original image with a SD-synthesized image. Static replacement with 20%-80% curation ratio corresponds to replacing images for one-four captions of

Figure 5: Effects of the amount of data curated when finetuning the captioning model. We can observe that Flickr30K needs more curation (40% ReplaceImg or 2 std Remove) than COCO (10% ReplaceImg or 1% ReplaceCap). Flickr30K benefits more from removing high-loss training samples, indicating the original dataset may be noisier than MS COCO. For the 2 std approach, the number of samples curated is not fixed after each epoch and varies between 5% to 10%.

Figure 6: Distribution of caption lengths.

the original five. The 50% replacement ratio mimics a fair coin-flip, where for each of the text-image samples, there is 50% probability for the image to be replaced by a synthesized image.

We compare the efficacy of these two approaches in Figure 7. When evaluating on the original 1k validation set, we see that for both approaches, incorporating synthesized images of 20% or 40% can assist finetuning and achieves higher BLEU4 and CIDEr scores. Nevertheless, dynamic image replacement consistently performs better than the static method, showing focusing on the hard samples is effective. For both replacement methods, performance starts to decrease when the curation ratio is too high. This may indicate that when incorporating too many images from the synthetic distribution, the gap increases between the training and evaluation sets.

Figure 8 shows the effect of the curation techniques in the training loss distributions across epochs. For the Remove approach, training samples with loss that are two standard deviations worse than the mean are dynamically removed during training, leading to the shrinking tail of the loss distribution. SD-based image replacement gradually reduces losses through learning from a mixture of Gaussian distribution from original image-text pairs and the ones contain synthesized images.

Figure 8: Loss distribution of training samples across epochs with different curation methods.

Figure 7: Dynamic image replacement against static replacement, as a function of the number of samples replaced.

Figure 9: Results of the human study of the errors made by the Stable Diffusion model in 100 images. The images used in the study were chosen to represent either low or high model loss. (a) Histogram of the number of errors annotated in each category. The most frequently occurring annotations concern weird deformations in the expected objects or humans. (b) Relationship between average number of identified errors by human annotations for each synthesized image and its captioning loss with regard to original captions. More errors are identified in images of higher loss. However, CLIPScore appears to fail in validating qualities of the synthesized images, as the score ranges are almost identical for samples that contain more errors.

### Human Study: Errors made by SD models

Finally, we conduct a human study of the errors present in the SD-synthesized images. This will serve to better understand any shortcomings with this approach that is not captured by automatic evaluation measures.

We first ranked SD-synthesized images by model loss from the 1K images in the validation set. This validation set of synthesized images was generated using the best performing configuration of the Stable Diffusion model (see Section 3.3). We then sampled a subset for human annotation using the top and bottom \(50\) images based on their loss using our fine-tuned captioning model. These images are uniformly divided into \(5\) sets, each containing \(20\) images with equal number of the high loss ones and the low loss ones. The data was annotated by 12 people, members of a university research lab with a basic understanding of Stable Diffusion but no knowledge of the bi-modal distribution of images. The annotators were asked to categorize the errors they observed in the synthesized images, given both the image and the reference sentences that were used to generate the images. Each participant annotated one set of 20 images.

Starting from the categories defined by van Miltenburg and Elliott [50], we predefined 25 categories including general errors such as color, or number mismatches, and errors related to people and objects in the images. Please see the user interface in supplemental materials. We analyze the human judgements for the images that have at least three annotations, yielding 74 unique images.

Figure 10: Examples of synthesized images that are of high losses (top) and examples of synthesized images that are of low losses (bottom). Human annotations show that consistent error types have been recognized for the high loss samples while CLIPScore fails to align with human judgement. The low loss synthesized images are visually less complicated than the higher loss ones, but can still often look weird and contain errors in color or objects.

As shown in Figure 8(a), the most common problem of SD-synthesized images are that they often generate weird face or body parts, which makes the images less natural or pleasant. The Stable Diffusion model is also weak at generating the correct number of people or objects. From Figure 8(b) we confirm the quality of our collected annotations that high loss figures often contain more errors on average. Furthermore, we note that CLIPScore does not appear to align with human judgements, indicating its weak capability of evaluating quality of generated images. Please see more concrete examples in Figure 10.

## 6 Conclusion

In this paper, we have shown a simple, yet effective, data curation framework that can improve the performance of image captioning models. We investigated three approaches to data curation that dynamically update the training dataset based on high-loss image-caption samples. The methods involved either removing a sample, replacing the caption in a sample, or generating a new image from existing captions. Experimental results on the Flickr30K and MS COCO datasets show the effectiveness of these approaches to data curation without increasing the total size of the training dataset. A deeper analysis of the images synthesized by Stable Diffusion shows frequent errors on generating objects of a certain amount or color, and struggles with human body features. A human evaluation of the errors in those images shows a clear difference in images with high or low losses.

In the future, we expect that better text-to-image generation models will lead to further improvements from using synthesized images for difficult captions in existing training datasets. We plan on verifying whether these findings extend to other image captioning models, which was not possible here due to computational issues. Finally, we are interested in applying the same framework to other multimodal tasks, especially those with undercomplete datasets that cannot comprehensively cover the distributional space due to the cost of crowdsourcing enough data, e.g. visual question answering, or visually-grounded dialog.

## Limitations

While our curation methods being effective on image-captioning in the finetuning and fewshot-learning settings, it is not clear if the same strategy can be scaled and adapted also to vision-language pretraining. Currently our data curation methods also rely on state-of-the art pretrained models for both image understanding and text-to-image generation. In pretraining, models will often be trained from scratch and pretraining data are often collected from multiple datasets and resources.

Moreover, while we take an online approach to data curation, our current approach is upper bounded in speed and performance of the text-to-image generation model. This might be a large bottle neck for adapting the strategy for more complicated vision-and-language tasks.

## Ethics Statement

Text-to-image generation with Stable Diffusion is controversial in the broader AI and ethics community[6]. For example, it can generate images according to gender or racial stereotypes, which may prove harmful to members of those communities [30]. In this paper, we use Stable Diffusion to improve the quality of an image captioning model, given a specific set of crowdsourced captions. Those captions may themselves contain harmful stereotypes that would become more prevalent in our dynamically updated training datasets. As we dynamically update the model with new images based on loss values, we remove the water-marker in our generated images to prevent information leak to the model. Use of the synthesized images will strictly follow community guidelines.

## References

* [1] Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. Do not have enough data? deep learning to the rescue! In _AAAI_, pages 7383-7390. AAAI Press, 2020.
** [2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In _European conference on computer vision_, pages 382-398. Springer, 2016.
* [3] Viktar Atlisha and Dmitrij Sesok. Text augmentation using bert for image captioning. _Applied Sciences_, 2020.
* [4] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J. Fleet. Synthetic data from diffusion models improves imagenet classification, 2023.
* [5] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=SlxSY2UZQT](https://openreview.net/forum?id=SlxSY2UZQT).
* [6] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. _arXiv preprint arXiv:2301.13188_, 2023.
* [7] Soravit Changpinyo, Doron Kukliansy, Idan Szpektor, Xi Chen, Nan Ding, and Radu Soricut. All you may need for VQA are image captions. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _NAACL_, pages 1947-1963. Association for Computational Linguistics, 2022.
* [8] Fuhai Chen, Rongrong Ji, Jinsong Su, Yongjian Wu, and Yunsheng Wu. Structcap: Structured semantic embedding for image captioning. In Qiong Liu, Rainer Lienhart, Haohong Wang, Sheng-Wei "Kuan-Ta" Chen, Susanne Boll, Yi-Ping Phoebe Chen, Gerald Friedland, Jia Li, and Shuicheng Yan, editors, _MM_, pages 46-54. ACM, 2017.
* [9] Fuhai Chen, Rongrong Ji, Xiaoshuai Sun, Yongjian Wu, and Jinsong Su. Groupcap: Group-based image captioning with structured relevance and diversity constraints. In _CVPR_, pages 1345-1353. Computer Vision Foundation / IEEE Computer Society, 2018.
* [10] Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation for any target language. In _Proceedings of the ninth workshop on statistical machine translation_, pages 376-380, 2014.
* [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).
* [12] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. A survey of data augmentation approaches for NLP. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 968-988, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.84. URL [https://aclanthology.org/2021.findings-acl.84](https://aclanthology.org/2021.findings-acl.84).
* [13] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard H. Hovy. A survey of data augmentation approaches for NLP. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _ACL_, volume ACL/IJCNLP 2021 of _Findings of ACL_, pages 968-988. Association for Computational Linguistics, 2021.
* [14] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, and Jianfeng Gao. Vision-language pre-training: Basics, recent advances, and future trends, 2022.
* [15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In _CVPR_, pages 10686-10696. IEEE, 2022.
** Guo et al. [2020] Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, and Hanqing Lu. Normalized and geometry-aware self-attention network for image captioning. In _CVPR_, pages 10324-10333. Computer Vision Foundation / IEEE, 2020.
* Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free evaluation metric for image captioning. In _EMNLP_, 2021.
* Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A reference-free evaluation metric for image captioning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7514-7528, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.595. URL [https://aclanthology.org/2021.emnlp-main.595](https://aclanthology.org/2021.emnlp-main.595).
* Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _NIPS_, 2017.
* Ho et al. [2022] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv:2204.03458_, 2022.
* Hooker [2020] Sara Hooker. The hardware lottery, 2020.
* Hossain et al. [2021] Md. Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, Hamid Laga, and Mohammed Bennamoun. Text to image synthesis for improved image captioning. _IEEE Access_, 9:64918-64928, 2021. doi: 10.1109/ACCESS.2021.3075579.
* Hu et al. [2022] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17980-17989, 2022.
* Huang et al. [2019] Lun Huang, Wenmin Wang, Jie Chen, and Xiaoyong Wei. Attention on attention for image captioning. In _ICCV_, pages 4633-4642. IEEE, 2019.
* Jain et al. [2023] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=99RpBVpLiX](https://openreview.net/forum?id=99RpBVpLiX).
* Jiang et al. [2018] Peng Jiang, Fanglin Gu, Yunhai Wang, Changhe Tu, and Baoquan Chen. Difnet: Semantic segmentation by diffusion networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL [https://proceedings.neurips.cc/paper_files/paper/2018/file/c2626d850c80ea07e7e7511bbae4c76f4b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2018/file/c2626d850c80ea07e7e7511bbae4c76f4b-Paper.pdf).
* Li et al. [2021] Guodun Li, Yuchen Zhai, Zehao Lin, and Yin Zhang. Similar scenes arouse similar emotions: Parallel data augmentation for stylized image captioning. In Heng Tao Shen, Yueting Zhuang, John R. Smith, Yang Yang, Pablo Cesar, Florian Metze, and Balakrishnan Prabhakaran, editors, _MM_, pages 5363-5372. ACM, 2021.
* Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.
* Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _ICML_, volume 162 of _Proceedings of Machine Learning Research_, pages 12888-12900. PMLR, 2022.
* Li et al. [2022] Minghui Li, Yan Wan, and Jinping Gao. What drives the ethical acceptance of deep synthesis applications? a fuzzy set qualitative comparative analysis. _Computers in Human Behavior_, 133:107286, 2022. ISSN 0747-5632. doi: [https://doi.org/10.1016/j.chb.2022.107286](https://doi.org/10.1016/j.chb.2022.107286).
* Li et al. [2021]* [31] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013).
* [32] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll'a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. _CoRR_, abs/1405.0312, 2014.
* [33] Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, and Soroush Vosoughi. Data boost: Text data augmentation through reinforcement learning guided conditional generation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _EMNLP_, pages 9031-9041. Association for Computational Linguistics, 2020.
* [34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022.
* [35] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Marina Meila and Tong Zhang, editors, _ICML_, volume 139, pages 8162-8171. PMLR, 2021.
* [36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _ICML_, volume 162, pages 16784-16804, 2022.
* [37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021.
* [39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.
* [40] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3982-3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).
* [41] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10674-10685, 2022.
* [42] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In Munkhtsetesg Nandigjav, Niloy J. Mitra, and Aaron Hertzmann, editors, _SIGGRAPH_, pages 15:1-15:10. ACM, 2022.
* [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.
** Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint_, 2022. URL [https://doi.org/10.48550/arXiv.2205.11487](https://doi.org/10.48550/arXiv.2205.11487).
* Schuhmann et al. [2021] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint_, 2021. URL [https://doi.org/10.48550/arXiv.2111.02114](https://doi.org/10.48550/arXiv.2111.02114).
* Sharma et al. [2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1238. URL [https://aclanthology.org/P18-1238](https://aclanthology.org/P18-1238).
* Shorten and Khoshgoftaar [2019] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. _Journal of big data_, 6(1):1-48, 2019.
* Singer et al. [2022] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022.
* Song et al. [2021] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.
* van Miltenburg and Elliott [2017] Emiel van Miltenburg and Desmond Elliott. Room for improvement in automatic image description: an error analysis. _CoRR_, abs/1704.04198, 2017.
* Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4566-4575, 2015.
* Villegas et al. [2022] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description, 2022.
* Wang et al. [2021] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. _Learning_, 2021.
* Wei and Zou [2019] Jason W. Wei and Kai Zou. EDA: easy data augmentation techniques for boosting performance on text classification tasks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _EMNLP-IJCNLP_, pages 6381-6387. Association for Computational Linguistics, 2019.
* Yang et al. [2020] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. G-daug: Generative data augmentation for commonsense reasoning. In Trevor Cohn, Yulan He, and Yang Liu, editors, _EMNLP_, volume EMNLP 2020 of _Findings of ACL_, pages 1008-1025. Association for Computational Linguistics, 2020.
* Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _TACL_, 2:67-78, 2014.
* Zhang et al. [2018] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _ICLR_. OpenReview.net, 2018.
* Zhou et al. [2020] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and VQA. In _AAAI_, pages 13041-13049. AAAI Press, 2020.