# Koopa: Learning Non-stationary Time Series

Dynamics with Koopman Predictors

Yong Liu, Chenyu Li, Jianmin Wang, Mingsheng Long

School of Software, BNRist, Tsinghua University, China

{liuyong21,lichenyu20}@mails.tsinghua.edu.cn, {jinwang,mingsheng}@tsinghua.edu.cn

Equal Contribution

###### Abstract

Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory that portrays complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by _Fourier Filter_ and design _Koopman Predictor_ to advance respective dynamics forward. Technically, we propose **Koopa** as a novel **Koopman** forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal neighborhood and is able to utilize incoming ground truth to scale up forecast horizon. Besides, by integrating Koopman Predictors into deep residual structure, we ravel out the binding reconstruction loss in previous Koopman forecasters and achieve end-to-end forecasting objective optimization. Compared with the state-of-the-art model, Koopa achieves competitive performance while saving \(77.3\%\) training time and \(76.0\%\) memory. Code is available at this repository: [https://github.com/thuml/Koopa](https://github.com/thuml/Koopa).

## 1 Introduction

Time series forecasting has become an essential part of real-world applications, such as weather forecasting, energy consumption, and financial assessment. With numerous available observations, deep learning approaches exhibit superior performance and bring the boom of deep forecasting models. TCNs [4, 43, 47] utilize convolutional kernels and RNNs [12, 22, 37] leverage the recurrent structure to capture underlying temporal patterns. Afterward, attention mechanism [42] becomes the mainstream of sequence modeling and Transformers [31, 48, 53] show great predictive power with the capability of learning point-wise temporal dependencies. And the recent revival of MLPs [32, 51, 52] presents a simple but effective approach to exhibit temporal dependencies by dense weighting.

In spite of elaboratively designed models, it is a fundamental problem for deep models to generalize on varying distribution [1, 25, 33], which is widely reflected in real-world time series because of inherent non-stationarity. Non-stationary time series is characterized by time-variant statistics and temporal dependencies in different periods [2, 14], inducing a huge distribution gap between training and inference and even among each lookback window. While previous methods [16, 28] tailor existing architectural design to attenuate the adverse effect of non-stationarity, few works research on the theoretical basis that can be applied to deal with time-variant temporal patterns naturally.

From another perspective, real-world time series acts like time-variant dynamics [6]. As one of the principal approaches to analyze complex dynamics, Koopman theory [20] provides an enlightenment to transform nonlinear system into measurement function space, which can be described by a linear Koopman operator. Several pilot works accomplish the integration with deep learning approaches by employing autoencoder networks [40] and operator-learning [26; 49]. More importantly, it is supported by Koopman theory that for time-variant dynamics, there exists a coordinate transformation of the system, where localized Koopman operators are valid to describe the whole measurement function space into several subspaces with linearization [23; 36]. Therefore, Koopman-based methods are appropriate to learn non-stationary time series dynamics (Figure 1). Besides, the linearity of measurement function space enables us to utilize spectral analysis to interpret nonlinear systems.

In this paper, we disentangle non-stationary series into time-invariant and time-variant dynamics and propose **Koopa** as a novel **Koopman** forecaster, which is composed of modular **K**oopman **P**redictors (**KP**) to hierarchically describe and advance forward series dynamics. Concretely, we utilize Fourier analysis for dynamics disentangling. And for time-invariant dynamics, the model learns Koopman embedding and linear operators to reveal the implicit transition underlying long-term series. As for the remaining time-variant components that exhibit strong locality, Koopa performs context-aware operator calculation and adaptation within different lookback windows. Besides, Koopman Predictor goes beyond the canonical design of Koopman Autoencoder without the binding reconstruction loss, and we incorporate modular blocks into deep residual architecture [32] to realize end-to-end time series forecasting. Our contributions are summarized as follows:

* From the perspective of modern dynamics Koopman theory, we propose _Koopa_ composed of modular _Fourier Filter_ and _Koopman Predictor_, which can hierarchically disentangle and exploit time-invariant and time-variant dynamics for time series forecasting.
* Based on the linearity of Koopman operators, the proposed model is able to utilize incoming series and adapt to varying dynamics for scaling up forecast horizon.
* Compared with state-of-the-art methods, our model achieves competitive performance while saving \(77.3\%\) training time and \(76.0\%\) memory averaged from six real-world benchmarks.

## 2 Related Work

### Time Series Forecasting with DNNs

Deep neural networks (DNNs) have made great breakthroughs in time series forecasting. TCN-based models [4; 43; 47] explore hierarchical temporal patterns and adopt shared convolutional kernels with diverse receptive fields. RNN-based models [12; 22; 37] utilize the recurrent structure with memory to reveal the implicit transition over time points. MLP-based models [32; 51; 52] learn point-wise weighting and the impressive performance and efficiency highlight that MLP performs well for modeling simple temporal dependencies. However, their practical applicability may still be constrained on non-stationary time series, which is endowed with time-variant properties and poses challenges for model capacity and efficiency. Unlike previous methods, Koopa fundamentally considers the complicated dynamics underlying time series and implements efficient and interpretable transition learners in both time-variant and time-invariant manners inspired by Koopman theory.

Recently, Transformer-based models have also achieved great success in time series forecasting. Initial attempts [19; 27; 48; 53] renovate the canonical structure and reduce the quadratic complexity

Figure 1: The measurement function \(g\) maps between non-stationary time series and the nonlinear dynamical system so that the timeline will correspond to a system trajectory. Therefore, time series variations in different periods are reflected as sub-regions of nonlinear dynamics, which can be portrayed and advanced forward in time by linear Koopman operators \(\{\mathcal{K}_{1},\mathcal{K}_{2},\mathcal{K}_{3}\}\) respectively.

for long-term forecasting. However, recent studies [16; 21] find it a central problem for Transformer and other DNNs to generalize on varying temporal distribution and several works [15; 16; 21; 28] tailor to empower the robustness against shifted distribution. Especially, PatchTST [31] boosts Transformer to the state-of-the-art performance by channel-independence and instance normalization [41] but may lead to unaffordable computational cost when the number of series variate is large. In this paper, our proposed model supported by Koopman theory works naturally for non-stationary time series and achieves the state-of-the-art forecasting performance with remarkable model efficiency.

### Learning Dynamics with Koopman Operator

Koopman theory [20] has emerged over decades as the dominant perspective to analyze modern dynamical systems [6]. Together with Dynamic Mode Decomposition (DMD) [38] as the leading numerical method to approximate the Koopman operator, significant advances have been accomplished in aerodynamics and fluid physics [3; 9; 30]. Recent progress made in Koopman theory is inherently incorporated with deep learning approaches in the data science era. Pilot works [29; 40; 50] leverage data-driven approaches such as Koopman Autoencoder to learn the measurement function and operator simultaneously. PCL [3] further introduces a backward procedure to improve the consistency and stability of the operator. Based on the capability of Koopman operator to advance nonlinear dynamics forward, it is also widely applied to sequence prediction. By means of Koopman spectral analysis, MDKAE [5] disentangles dominant factors underlying sequential data and is competent to forecast with specific factors. K-Forecast [24] utilizes Koopman theory to handle nonlinearity in temporal signals and propose to optimize data-dependent basis for long-term time series forecasting. By leveraging predefined measurement functions, KNF [44] learns Koopman operator and attention map to cope with time series forecasting with changing temporal distribution.

Different from previous Koopman forecasters, we design modular Koopman Predictors to tackle time-variant and time-invariant components with hierarchically learned operators, and renovate Koopman Autoencoder by removing the reconstruction loss to achieve fully predictive training.

## 3 Background

### Koopman Theory

A discrete-time dynamical system can be formulated as \(x_{t+1}=\mathbf{F}(x_{t})\), where \(x_{t}\) denotes the system state and \(\mathbf{F}\) is a vector field describing the dynamics. However, it is challenging to identify the system transition directly on the state because of nonlinearity or noisy data. Instead, Koopman theory [20] hypothesizes the state can be projected into the space of measurement function \(g\), which can be governed and advanced forward in time by an infinite-dimensional linear operator \(\mathcal{K}\), such that:

\[\mathcal{K}\circ g(x_{t})=g\big{(}\mathbf{F}(x_{t})\big{)}=g(x_{t+1}). \tag{1}\]

Koopman theory provides a bridge between finite-dimensional nonlinear dynamics and infinite-dimensional linear dynamics, where spectral analysis tools can be applied to obtain in-depth analysis.

### Dynamic Mode Decomposition

Dynamic Mode Decomposition (DMD) [38] seeks the best fitted finite-dimensional matrix \(K\) to approximate infinite-dimensional operator \(\mathcal{K}\) by collecting the observed system state (a.k.a. _snapshot_). Although DMD is the standard numerical method to analyze dynamics, it only works on linear space assumptions, which can be hardly identified without prior knowledge. Therefore, eDMD [45] is proposed to avoid handcrafting measurement functions and harmonious incorporations are made with the learning approach by employing autoencoders, which yields Koopman Autoencoder (KAE). By the universal approximation theorem [13] of deep networks, KAE finds desired _Koopman embedding_\(g(x_{t})\) with learned measurement function in a data-driven approach.

### Time Series as Dynamics

It is challenging to predict real-world time series because of inherent non-stationarity. But if we zoom in the timeline, we will find the localized time series exhibited weak stationarity. It coincides with Koopman theory to analyze large nonlinear dynamics. That is, the measurement function space can be divided into several neighborhoods, which are discriminately portrayed by localized linear operators [23]. Therefore, we leverage Koopman-based approaches that tackle large nonlinear dynamics by disentangling time-variant and time-invariant dynamics. Inspired by Wold's Theorem [46] that every covariance-stationary time series \(X_{t}\) can be formally decomposed as:

\[X_{t}=\eta_{t}+\sum_{j=0}^{\infty}b_{j}\varepsilon_{t-j}, \tag{2}\]

where \(\eta_{t}\) denotes the deterministic component and \(\varepsilon_{t}\) is the stochastic component as the stationary process input of linear filter \(\{b_{j}\}\), we introduce globally learned and localized linear Koopman operators to exploit respective dynamics underlying different components.

## 4 Koopa

We propose _Koopa_ composed of stackable _Koopa Blocks_ (Figure 2). Each block is obliged to learn the input dynamics and advance it forward for prediction. Instead of struggling to seek one unified operator that governs the whole measurement function space, each Koopa Block is encouraged to learn operators hierarchically by taking the residual of previous block fitted dynamics as its input.

Koopa BlockAs aforementioned, it is essential to disentangle different dynamics and adopt proper operators for non-stationary series forecasting. The proposed block shown in Figure 2 contains _Fourier Filter_ that utilizes frequency domain statistics to disentangle time-variant and time-invariant components and implements two types of _Koopman Predictor (KP)_ to obtain Koopman embedding respectively. In Time-invariant KP, we set the operator as a model parameter to be globally learned from lookback-forecast windows. In Time-variant KP, analytical operator solutions are calculated locally within the lookback window, with series segments arranged as snapshots. In detail, we formulate the \(b\)-th block input \(X^{(b)}\) as \([x_{1},x_{2},\ldots,x_{T}]^{\top}\in\mathbb{R}^{T\times C}\), where \(T\) and \(C\) denote the lookback window length and the variate number. The target is to output a forecast window of length \(H\). Our proposed Fourier Filter conducts disentanglement at the beginning of each block:

\[X^{(b)}_{\text{var}},\;X^{(b)}_{\text{inv}}=\mathrm{FourierFilter}(X^{(b)}). \tag{3}\]

Respective KPs will predict with time-invariant input \(X^{(b)}_{\text{inv}}\) and time-variant input \(X^{(b)}_{\text{var}}\), and Time-variant KP simultaneously outputs the fitted input \(\hat{X}^{(b)}_{\text{var}}\):

\[\begin{split} Y^{(b)}_{\text{inv}}&=\mathrm{TimeInvKP }(X^{(b)}_{\text{inv}}),\\ \hat{X}^{(b)}_{\text{var}},Y^{(b)}_{\text{var}}&= \mathrm{TimeVarKP}(X^{(b)}_{\text{var}}).\end{split} \tag{4}\]

Figure 2: Left: Koopa structure. By taking the residual of previous block fitted dynamical system, each block learns hierarchical dynamics and Koopa aggregates the forecast of all blocks. Right: For time-invariant forecast, Koopa learns globally shared dynamics from each lookback-forecast window pair. For time-variant forecast, the model calculates localized and segment-wise dynamics.

Unlike KAEs [45; 44] that introduce a loss term for rigorous reconstruction of the lookback-window series, we feed the residual \(X^{(b+1)}\) as the input of next block for learning a corrective operator. And the model forecast \(Y\) is the sum of predicted components \(Y^{(b)}_{\text{var}},Y^{(b)}_{\text{inv}}\) gathered from all Koopa Blocks:

\[X^{(b+1)}=X^{(b)}_{\text{var}}-\hat{X}^{(b)}_{\text{var}},\;\;Y=\sum\big{(}Y^{ (b)}_{\text{var}}+Y^{(b)}_{\text{inv}}\big{)}. \tag{5}\]

Fourier FilterTo disentangle the series components, we leverage Fourier analysis to find the globally shared and localized frequency spectrums reflected on different periods. Concretely, we precompute the Fast Fourier Transform (FFT) of each lookback window of the training set, calculate the averaged amplitude of each spectrum \(\mathcal{S}=\{0,1,\ldots,[T/2]\}\), and sort them by corresponding amplitude. We take the top percent of \(\alpha\) as the subset \(\mathcal{G}_{\alpha}\subset\mathcal{S}\), which contains dominant spectrums shared among all lookback windows and exhibits time-invariant dynamics underlying the dataset. And the remaining spectrums are the specific ingredient for varying windows in different periods. Therefore, we divide the spectrums \(\mathcal{S}\) into \(\mathcal{G}_{\alpha}\) and its complementary set \(\mathcal{\bar{G}}_{\alpha}\). During training and inference, \(\operatorname{FourierFilter}(\cdot)\) conducts the disentanglement of input \(X\) (block superscript omitted) as

\[X_{\text{inv}} =\mathcal{F}^{-1}\big{(}\operatorname{Filter}\big{(}\mathcal{G}_{ \alpha},\;\mathcal{F}(X)\big{)}\big{)}, \tag{6}\] \[X_{\text{var}} =\mathcal{F}^{-1}\big{(}\operatorname{Filter}\big{(}\mathcal{ \bar{G}}_{\alpha},\;\mathcal{F}(X)\big{)}\big{)}=X-X_{\text{inv}},\]

where \(\mathcal{F}\) means FFT, \(\mathcal{F}^{-1}\) is its inverse and \(\operatorname{Filter}(\cdot)\) only passes corresponding frequency spectrums with the given set. We validate the disentangling effect of our proposed Fourier Filter in Section 5.2 by calculating the variation degree of temporal dependencies in the disentangled series.

Time-invariant KPTime-invariant KP is designed to portray the globally shared dynamics, which discovers the direct transition from lookback window to forecast window as \(\mathbf{F}:X_{\text{inv}}\mapsto Y_{\text{inv}}\). Concretely, we introduce a pair of \(\operatorname{Encoder}:\mathbb{R}^{T\times C}\mapsto\mathbb{R}^{D}\) and \(\operatorname{Decoder}:\mathbb{R}^{D}\mapsto\mathbb{R}^{H\times C}\) to learn the common Koopman embedding for the time-invariant components of running window pairs, where \(D\) denotes the embedding dimension. Working on the data-driven measurement function, we introduce the operator \(K_{\text{inv}}\in\mathbb{R}^{D\times D}\) as a learnable parameter in each Time-invariant KP, which regards the embedding of lookback and forecast window \(Z_{\text{back}},Z_{\text{fore}}\in\mathbb{R}^{D}\) as running snapshot pairs. The procedure is shown in Figure 3 and \(\operatorname{TimeInvKP}(\cdot)\) is formulated as follows:

\[Z_{\text{back}}=\operatorname{Encoder}(X_{\text{inv}}),\;Z_{\text{fore}}=K_{ \text{inv}}Z_{\text{back}},\;Y_{\text{inv}}=\operatorname{Decoder}(Z_{\text{ fore}}). \tag{7}\]

Figure 3: Left: Time-invariant KP learns Koopman embedding and operator with time-invariant components globally from all windows. Right: Time-variant KP conducts localized operator calculation within lookback window and advances dynamics forward with the obtained operator for predictions.

Time-variant KPAs time-variant dynamics changes continuously, we utilize localized snapshots in a window, which constitute a temporal neighborhood more likely to be linearized. To obtain semantic snapshots and reduce iterations, the input \(X_{\text{var}}\) is divided into \(\frac{T}{S}\) segments \(\mathbf{x}_{j}\) of length \(S\):

\[\mathbf{x}_{j}=[x_{(j-1)S+1},\dots,x_{jS}]^{\top}\in\mathbb{R}^{S\times C},\;j= 1,2,\dots,T/S. \tag{8}\]

We assume \(S\) is divisible by \(T\) and \(H\); otherwise, we pad the input or truncate the output to make it compatible. Time-variant KP aims to portray localized dynamics, which is manifested analytically as the segment-wise transition \(\mathbf{F}:\mathbf{x}_{t}\mapsto\mathbf{x}_{t+1}\) with observed snapshots. We utilize another pair of \(\mathrm{Encoder}:\mathbb{R}^{S\times C}\mapsto\mathbb{R}^{D}\) to transform each segment into Koopman embedding \(z_{j}\) and \(\mathrm{Decoder}:\mathbb{R}^{D}\mapsto\mathbb{R}^{S\times C}\) to transform the fitted or predicted embedding \(\hat{z}_{j}\) back to time segments \(\hat{\mathbf{x}}_{j}\):

\[z_{j}=\mathrm{Encoder}(\mathbf{x}_{j}),\;\hat{\mathbf{x}}_{j}=\mathrm{Decoder }(\hat{z}_{j}). \tag{9}\]

Given snapshots collection \(Z=[z_{1},\dots,z_{\frac{T}{S}}]\in\mathbb{R}^{D\times\frac{T}{S}}\), we leverage eDMD [45] to find the best fitted matrix that advances forward the system. We apply one-step operator approximation as follows:

\[Z_{\text{back}}=[z_{1},z_{2},\dots,z_{\frac{T}{S}-1}],\;Z_{\text{fore}}=[z_{2},z_{3},\dots,z_{\frac{T}{S}}],\;K_{\text{var}}=Z_{\text{fore}}Z_{\text{back}}^ {\dagger}, \tag{10}\]

where \(Z_{\text{back}}^{\dagger}\in\mathbb{R}^{(\frac{T}{S}-1)\times D}\) is the Moore-Penrose inverse of lookback window embedding collection. The calculated \(K_{\text{var}}\in\mathbb{R}^{D\times D}\) varies with windows and helps to analyze local temporal variations as a linear system. With the calculated operator, the fitted embedding is formulated as follows:

\[[\hat{z}_{1},\hat{z}_{2},\dots,\hat{z}_{\frac{T}{S}}]=[z_{1},K_{\text{var}}z_{ 1},\dots,K_{\text{var}}z_{\frac{T}{S}-1}]=[z_{1},K_{\text{var}}Z_{\text{back}}]. \tag{11}\]

To obtain a prediction of length \(H\), we iterate operator forwarding to get \(\frac{H}{S}\) predicted embedding:

\[\hat{z}_{\frac{T}{S}+t}=(K_{\text{var}})^{t}z_{\frac{T}{S}},\;\;t=1,2,\dots,H/S. \tag{12}\]

Finally, we arrange the segments transformed by \(\mathrm{Decoder}(\cdot)\) as the module outputs \(\hat{X}_{\text{var}},Y_{\text{var}}\). The whole procedure is shown in Figure 3 and \(\mathrm{TimeVarKP}(\cdot)\) can be formulated as Equation 8-13.

\[\hat{X}_{\text{var}}=[\hat{\mathbf{x}}_{1},\dots,\hat{\mathbf{x}}_{\frac{T}{S} }]^{\top},\;Y_{\text{var}}=[\hat{\mathbf{x}}_{\frac{T}{S}+1},\dots,\hat{ \mathbf{x}}_{\frac{T}{S}+\frac{H}{S}}]^{\top}. \tag{13}\]

Forecasting ObjectiveIn Koopa, Encoder, Decoder and \(K_{\text{inv}}\) are learnable parameters, while \(K_{\text{var}}\) is calculated on-the-fly. To maintain the Koopman embedding consistency in different blocks, we share \(\mathrm{Encoder}\), \(\mathrm{Decoder}\) in Time-variant and Time-invariant KPs, which are formulated as \(\phi_{\text{var}}\) and \(\phi_{\text{inv}}\) respectively, and use the MSE loss with the ground truth \(Y_{\text{gt}}\) for parameter optimization:

\[\mathrm{argmin}_{K_{\text{var}},\phi_{\text{var}},\phi_{\text{inv}}}\;\mathcal{ L}_{\text{MSE}}(Y,\;Y_{\text{gt}}). \tag{14}\]

Optimizing by a single forecasting objective based on the assumption that if reconstruction failed, the prediction must also fail. Thus eliminating forecast discrepancy helps for fitting observed dynamics.

## 5 Experiments

DatasetsWe conduct extensive experiments to evaluate the performance and efficiency of Koopa. For multivariate forecasting, we include six real-world benchmarks used in Autoformer [48]: ECL (UCI), ETT [53], Exchange [22], ILI (CDC), Traffic (PeMS), and Weather (Wetterstation). For univariate forecasting, we evaluate the performance on the well-acknowledged M4 dataset [39], which contains four subsets of periodically collected univariate marketing data. And we follow the data processing and split ratio used in TimesNet [47].

Notably, instead of setting a fixed lookback window length, for every forecast window length \(H\), we set the length of lookback window \(T=2H\) as the same with N-BEATS [32], because historical observations are always available in real-world scenarios and it can be beneficial for deep models to leverage more observed data with the increasing forecast horizon.

BaselinesWe extensively compare Koopa with the state-of-the-art deep forecasting models, including Transformer-based model: Autoformer [48], PatchTST [31]; TCN-based model: TimesNet [47], MICN [43]; MLP-based model: DLinear [51]; Fourier forecaster: FiLM [54], and Koopman forecaster: KNF [44]. We also introduce additional specialized models N-HiTS [7] and N-BEATS [32] for univariate forecasting as competitive baselines. All the baselines we reproduced are implemented based on the original paper or official code. We repeat each experiment three times with different random seeds and report the test MSE/MAE. And we provide detailed code implementation and hyperparameters sensitivity in Appendix C.

### Time Series Forecasting

Forecasting resultsWe list the results in Table 1-2 with the best in **bold** and the second underlined. Koopa shows competitive forecasting performance in both multivariate and univariate forecasting. Concretely, Koopa achieves state-of-the-art performance in more than **70%** multivariate settings and consistently outperforms other deep models in the univariate settings.

Notably, Koopa surpasses the state-of-the-art Koopman-based forecaster KNF by a large margin in real-world time series, which can be attributed to our hierarchical dynamics learning and disentangling mechanism. Also, as the representative of efficient linear models, the performance of DLinear is still subpar in ILI, Traffic and Weather, indicating that nonlinear dynamics underlying the time series poses challenges for model capacity and point-wise weighting may not be appropriate to portray time-variant dynamics. Besides, compared with painstakingly trained PatchTST with channel-independence mechanism, our model can achieve a close and even better performance with naturally addressed non-stationary properties of real-world time series.

Model efficiencyWe comprehensively evaluate the model efficiency from three aspects: forecasting performance, training speed, and memory footprint. In Figure 4, we compare the efficiency under two representative datasets with different variate numbers (7 in ETTh2 and 862 in Traffic).

Compared with the state-of-the art forecasting model PatchTST, Koopa saves **62.3%** and **96.5%** training time respectively in the ETTh2 and Traffic datasets with only **26.8%** and **2.9%** memory footprint. Concretely, the averaged training time and memory ratio of Koopa compared to PatchTST are **22.7%** and **24.0%** in all six datasets (see Appendix D.3 for the detail). Besides, as an efficient MLP-based forecaster, Koopa is also capable of learning nonlinear dynamics from time-variant and time-invariant components, and thus achieves a better performance.

### Model Analysis

Dynamics disentanglementTo validate the disentangling effect of our proposed Fourier Filter, we divide the whole time series into 20 subsets of different periods and conduct respective linear regression on the components disentangled by Fourier Filter. The standard deviation of the linear weighting reflects the variation of point-to-point temporal dependencies, which works as the manifestation of time-variant property. We plot the value as _Degree of Variation_ (Figure 5 Left). It can be observed that larger deviations occur in the time-variant component, which indicates the proposed module successfully disentangles two types of dynamics from the perspective of frequency domain.

Case studyWe present a case study on real-world time series (exchange rate) on the right of Figure 5. We sample the lookback window at the interval of one year and visualize the Koopman operators calculated in Time-variant KP. It can be clearly observed that localized operators can exhibit changing temporal patterns in different periods, indicating the necessity of utilizing varying operators to describe time-variant dynamics. And interpretable insights are also presented as series uptrends correspond to heatmaps with large value and downtrends are reflected with small value.

Figure 4: Model efficiency comparison. The performance comes from Table 1 with forecast window length \(H=144\). Training time and memory footprint are recorded with the same batch size and official code configuration. Full results of all six datasets are provided in Appendix D.3.

Figure 5: Left: Comparison of Degree of Variation (the standard deviation of linear weighting fitted on different periods), we plot respective values of disengaged components on all six datasets. Right: A case of localized Koopman operators calculated on the Exchange dataset at the interval of one year.

Ablation studyWe conduct ablations on Koopan. As shown in Table 3, Time-variant and Time-invariant KPs perform as complementary modules to explore the dynamics underlying time series, and discarding any one of them will lead to the inferior performance. Besides, we evaluate alternative decomposition filters to disentangle time series dynamics. We find the proposed Fourier Filter conducts effective disentanglement, where the amplitude statistics of frequency spectrums from different periods are utilized to exhibit time-agnostic information. Therefore, Koopa tackling the right dynamics with complementary modules can achieves the best performance.

Avoiding rigorous reconstructionUnlike previous Koopman Autoencoders, the proposed Koopman Predictor does not reconstruct the whole dynamics at once, but aims to portray the partial dynamics evolution. Thus we remove the reconstruction branch, which is only utilized during training in previous KAEs. In our deep residual structure, the predictive objective function works as a good optimization indicator. We validate the design in Table 4, where the performance of sorely forecasting objective optimized model is better than with an additional reconstruction loss. Because the end-to-end forecasting objective helps to reduce the optimization gap between training and inference, making it a valuable contribution of applying Koopman operators on end-to-end time series forecasting.

Learning stable operatorsWe turn to analyze our architectural design from the spectral perspective. The eigenvalues of the operator determine the amplitude of dynamics evolution. As most of non-stationary time series experience the distribution shift and can be regarded as an unstable evolution, the learned Koopman operator with the modulus far from the unit circle will cause non-divergent and even explosive trending in the long term, leading to training failures.

To tackle this problem generally faced by Koopman-based forecasters, we propose to utilize the disentanglement and deep residual structure. We measure the stability of the operator as the average distance of eigenvalues from the unit circle. As shown in Figure 6, the operator can become more stable by the above two techniques. The disentanglement helps to describe complex dynamics based on the decomposition and appropriate inductive bias can be applied. The architecture where each block is employed to fill the residual of the previously fitted dynamics reduces the difficulty of directly reconstructing complicated dynamics. Each block portrays the basic process driven by a stable operator within its power, which can be aggregated for a complicated non-stationary process.

### Scaling Up Forecast Horizon

Most deep forecasting models work as a settled function once trained (e.g. input-\(T\)-output-\(H\)). For scenarios where the prediction horizon is mismatched or long-term, it poses two challenges for the trained model: (1) reuse parameters learned from observed series; (2) utilize incoming ground truth for model adaptation. The practical scenarios, which we name as _scaling up forecast horizon_, may

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c}{ETTh2} & \multicolumn{2}{c}{Exchange} & \multicolumn{2}{c}{ECL} & \multicolumn{2}{c}{Traffic} & \multicolumn{2}{c}{Weather} & \multicolumn{2}{c}{ILI} \\ \cline{2-13} Model & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline
**Koopa** & **0.303** & **0.356** & **0.110** & **0.230** & **0.143** & **0.243** & **0.404** & **0.277** & **0.161** & **0.210** & **1.734** & **0.862** \\ KAE & 0.312 & 0.361 & 0.129 & 0.248 & 0.169 & 0.269 & 0.463 & 0.329 & 0.170 & 0.217 & 2.189 & 0.974 \\ \hline Promotion & \multicolumn{2}{c|}{2.88\%} & \multicolumn{2}{c|}{14.73\%} & \multicolumn{2}{c|}{15.38\%} & \multicolumn{2}{c|}{12.74\%} & \multicolumn{2}{c|}{5.29\%} & \multicolumn{2}{c}{20.79\%} \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance comparison of the dynamics learning blocks implemented by our proposed Koopman Predictor (_Koopa_) and the canonical Koopman Autoencoder [29](_KAE_).

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c}{ECL} & \multicolumn{2}{c}{ETTh2} & \multicolumn{2}{c}{Exchange} & \multicolumn{2}{c}{ILI} & \multicolumn{2}{c}{Traffic} & \multicolumn{2}{c}{Weather} \\ \cline{2-13} Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline Only \(K_{\text{inv}}\) & 0.148 & 0.250 & 0.312 & 0.358 & 0.120 & 0.241 & 2.146 & 0.963 & 0.740 & 0.446 & 0.170 & 0.213 \\ Only \(K_{\text{var}}\) & 1.547 & 0.782 & 0.371 & 0.405 & 0.205 & 0.316 & 2.370 & 1.006 & 0.947 & 0.544 & 0.180 & 0.232 \\ Truncated Filter & 0.155 & 0.255 & 0.311 & 0.362 & 0.129 & 0.246 & 1.988 & 0.907 & 0.536 & 0.334 & 0.172 & 0.220 \\ Branch Switch & 0.696 & 0.393 & 0.344 & 0.385 & 0.231 & 0.325 & 2.130 & 0.964 & 0.451 & 0.304 & 0.173 & 0.221 \\
**Koopa** & **0.146** & **0.246** & **0.303** & **0.356** & **0.111** & **0.230** & **1.734** & **0.862** & **0.419** & **0.293** & **0.162** & **0.211** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Model ablation. _Only \(K_{\text{inv}}\)_ uses one-block Time-invariant KP; _Only \(K_{\text{var}}\)_ stacks Time-variant KPs only; _Truncated Filter_ replaces Fourier Filter with High-Low Pass Filter; _Branch Switch_ changes the order of KPs on disentangled components. The averaged results are listed here.

lead to failure on most deep models but can be naturally tackled by Koopa. In detail, we first train Koopa with forecast length \(H_{\text{tr}}\) and attempt to apply it on a larger forecast length \(H_{\text{te}}\).

MethodKoopa scales up forecast horizon as follows: Since Time-invariant KP has learned the globally shared dynamics and Time-variant KP can calculate localized operator \(K_{\text{var}}\) within the lookback window, we freeze the parameters of trained Koopa but only use the incoming ground truth to adapt \(K_{\text{var}}\). The naive implementation uses incremental Koopman embedding with dimension \(D\) and conducts Equation 10 to obtain an updated operator, which has a complexity of \(\mathcal{O}(H_{\text{te}}D^{3})\). We further propose an iterative algorithm with improved \(\mathcal{O}((H_{\text{te}}+D)D^{2})\) complexity. The detailed method implementations and complexity analysis can be found in Appendix A.

ResultsAs shown in Table 5, the proposed operator adaption mechanism further boosts the performance on the scaling up scenario, which can be attributed to more accurately fitted time-variant dynamics with incoming ground truth snapshots. Besides, the promotion becomes more significant when applied to non-stationary datasets (manifested as large ADF Test Statistic [10]).

## 6 Conclusion

This paper tackles time series as dynamical systems. With disentangled time-variant and time-invariant components from non-stationary series, the Koopa model reveals the complicated dynamics hierarchically and leverages MLP modules to learn Koopman embedding and operator. Experimentally, our model shows competitive performance with remarkable efficiency and the potential to scale up the forecast length by operator adaptation. In the future, we will explore Koopa with the dynamic modes underlying non-stationary data using the toolbox of Koopman spectral analysis.

## Acknowledgments

This work was supported by the National Key Research and Development Plan (2021YFC3000905), National Natural Science Foundation of China (62022050 and 62021002), Beijing Nova Program (Z201100006820041), and BNRist Innovation Fund (BNR2021RC01002).

Figure 6: Visualization of the operator stability on the highly non-stationary Exchange dataset. We plot the first block time-invariant operator eigenvalues of the following design: (a) Single-block model with only time-invariant operator. (b) Single-block model with time-invariant and time-variant operators. (c) Two-block model with time-invariant and time-variant operators.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c|}{Exchange} & \multicolumn{2}{c|}{ETTh2} & \multicolumn{2}{c|}{ILI} & \multicolumn{2}{c|}{ECL} & \multicolumn{2}{c|}{Traffic} & \multicolumn{2}{c}{Weather} \\ ADF Test Statistic & \multicolumn{2}{c|}{(-1.889)} & \multicolumn{2}{c|}{(-4.135)} & \multicolumn{2}{c|}{(-5.406)} & \multicolumn{2}{c|}{(-8.483)} & \multicolumn{2}{c|}{(-15.046)} & \multicolumn{2}{c}{(-26.661)} \\ \hline Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline Koopa & 0.214 & 0.348 & 0.437 & 0.429 & 2.836 & 1.065 & 0.199 & 0.298 & 0.709 & 0.437 & 0.237 & 0.276 \\
**Koopa OA** & **0.172** & **0.319** & **0.372** & **0.404** & **2.427** & **0.907** & **0.182** & **0.271** & **0.699** & **0.426** & **0.225** & **0.264** \\ \hline Promotion (MSE) & 19.6\% & 14.9\% & 14.1\% & 8.5\% & 1.4\% & 5.1\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: Scaling up forecast horizon: \((H_{\text{tr}},H_{\text{te}})=(24,48)\) for ILI and \((H_{\text{tr}},H_{\text{te}})=(48,144)\) for others. _Koopa_ conducts vanilla rolling forecast and _Koopa OA_ further introduces operator adaptation.

## References

* [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. _NeurIPS_, 2021.
* [2] O. Anderson and M. Kendall. Time-series. 2nd edn. _J. R. Stat. Soc. (Series D)_, 1976.
* [3] Omri Azencot, N Benjamin Erichson, Vanessa Lin, and Michael Mahoney. Forecasting sequential data using consistent koopman autoencoders. In _International Conference on Machine Learning_, pages 475-485. PMLR, 2020.
* [4] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. _arXiv preprint arXiv:1803.01271_, 2018.
* [5] Nimrod Berman, Ilan Naiman, and Omri Azencot. Multifactor sequential disentanglement via structured koopman autoencoders. _arXiv preprint arXiv:2303.17264_, 2023.
* [6] Steven L Brunton, Marko Budisic, Eurika Kaiser, and J Nathan Kutz. Modern koopman theory for dynamical systems. _arXiv preprint arXiv:2102.12086_, 2021.
* [7] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. _arXiv preprint arXiv:2201.12886_, 2022.
* [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [9] Hamidreza Eivazi, Luca Guastoni, Philipp Schlatter, Hossein Azizpour, and Ricardo Vinuesa. Recurrent neural networks and koopman-based frameworks for temporal predictions in a low-order model of turbulence. _International Journal of Heat and Fluid Flow_, 90:108816, 2021.
* [10] Graham Elliott, Thomas J Rothenberg, and James H Stock. Efficient tests for an autoregressive unit root, 1992.
* [11] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. _arXiv preprint arXiv:1312.6211_, 2013.
* [12] S. Hochreiter and J. Schmidhuber. Long short-term memory. _Neural Comput._, 1997.
* [13] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. _Neural networks_, 4(2):251-257, 1991.
* [14] Rob J Hyndman and George Athanasopoulos. _Forecasting: principles and practice_. OTexts, 2018.
* [15] Xiaoyong Jin, Youngsuk Park, Danielle Maddix, Hao Wang, and Yuyang Wang. Domain adaptation for time series forecasting via attention sharing. In _International Conference on Machine Learning_, pages 10280-10297. PMLR, 2022.
* [16] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In _ICLR_, 2022.
* [17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* [19] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _ICLR_, 2020.
* [20] Bernard O Koopman. Hamiltonian systems and transformation in hilbert space. _Proceedings of the National Academy of Sciences_, 17(5):315-318, 1931.
* [21] Vitaly Kuznetsov and Mehryar Mohri. Discrepancy-based theory and algorithms for forecasting non-stationary time series. _Annals of Mathematics and Artificial Intelligence_, 88(4):367-399, 2020.

* [22] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In _SIGIR_, 2018.
* [23] Yueheng Lan and Igor Mezic. Linearization in the large of nonlinear systems and koopman operator spectrum. _Physica D: Nonlinear Phenomena_, 242(1):42-53, 2013.
* [24] Henning Lange, Steven L Brunton, and J Nathan Kutz. From fourier to koopman: Spectral methods for long-term time series prediction. _The Journal of Machine Learning Research_, 22(1):1881-1918, 2021.
* [25] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In _ICCV_, 2017.
* [26] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* [27] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In _ICLR_, 2021.
* [28] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Rethinking the stationarity in time series forecasting. _arXiv preprint arXiv:2205.14415_, 2022.
* [29] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. _Nature communications_, 9(1):4950, 2018.
* [30] Jeremy Morton, Antony Jameson, Mykel J Kochenderfer, and Freddie Witherden. Deep dynamical modeling and control of unsteady fluid flows. _Advances in Neural Information Processing Systems_, 31, 2018.
* [31] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. _arXiv preprint arXiv:2211.14730_, 2022.
* [32] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. _ICLR_, 2019.
* [33] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. _TKDE_, 2009.
* [34] Adam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_, 2019.
* [35] Quang Pham, Chenghao Liu, Doyen Sahoo, and Steven CH Hoi. Learning fast and slow for online time series forecasting. _arXiv preprint arXiv:2202.11672_, 2022.
* [36] Joshua L Proctor, Steven L Brunton, and J Nathan Kutz. Dynamic mode decomposition with control. _SIAM Journal on Applied Dynamical Systems_, 15(1):142-161, 2016.
* [37] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. _International Journal of Forecasting_, 36(3):1181-1191, 2020.
* [38] Peter J Schmid. Dynamic mode decomposition of numerical and experimental data. _Journal of fluid mechanics_, 656:5-28, 2010.
* [39] Spyros Makridakis. M4 dataset, 2018.
* [40] Naoya Takeishi, Yoshinobu Kawahara, and Takehisa Yairi. Learning koopman invariant subspaces for dynamic mode decomposition. _Advances in neural information processing systems_, 30, 2017.
* [41] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. _arXiv preprint arXiv:1607.08022_, 2016.
* [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* [43] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Mcin: Multi-scale local and global context modeling for long-term series forecasting. In _The Eleventh International Conference on Learning Representations_, 2023.

* [44] Rui Wang, Yihe Dong, Sercan O Arik, and Rose Yu. Koopman neural forecaster for time series with temporal distribution shifts. _arXiv preprint arXiv:2210.03675_, 2022.
* [45] Matthew O Williams, Ioannis G Kevrekidis, and Clarence W Rowley. A data-driven approximation of the koopman operator: Extending dynamic mode decomposition. _Journal of Nonlinear Science_, 25:1307-1346, 2015.
* [46] Herman Wold. _A study in the analysis of stationary time series_. PhD thesis, Almqvist & Wiksell, 1938.
* [47] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. _arXiv preprint arXiv:2210.02186_, 2022.
* [48] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting. In _NeurIPS_, 2021.
* [49] Wei Xiong, Xiaomeng Huang, Ziyang Zhang, Ruixuan Deng, Pei Sun, and Yang Tian. Koopman neural operator as a mesh-free solver of non-linear partial differential equations. _arXiv preprint arXiv:2301.10022_, 2023.
* [50] Enoch Yeung, Soumya Kundu, and Nathan Hodas. Learning deep neural network representations for koopman operators of nonlinear dynamical systems. In _2019 American Control Conference (ACC)_, pages 4832-4839. IEEE, 2019.
* [51] Ailing Zeng, Muki Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? _arXiv preprint arXiv:2205.13504_, 2022.
* [52] Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. _arXiv preprint arXiv:2207.01186_, 2022.
* [53] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _AAAI_, 2021.
* [54] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. _Advances in Neural Information Processing Systems_, 35:12677-12690, 2022.

## Appendix A Scaling Up Forecast Horizon

In this section, we introduce the capability of Koopa to scale up forecast horizon. In detail, we train a Koopa model with forecast length \(H_{\text{tr}}\) and attempt to apply it on a larger length \(H_{\text{tr}}\). The basic approach conducts rolling forecast by taking the model prediction as the input of the next iteration until the desired forecast horizon is all filled. Instead, we further assume that after the model gives a prediction, the model can utilize the incoming ground truth for _model adaptation_ and continue rolling forecast for the next iteration. It is notable that we do not retrain parameters during model adaptation, since it will lead to overfitting on the incoming ground truth and Catastrophic Forgetting [11, 18, 35].

Koopa can naturally cope with the scenario by learning Koopman embedding and operator \(K_{\text{inv}}\) in Time-invariant KPs while calculating localized operator \(K_{\text{var}}\) to describe the dynamics in the temporal neighborhood. Therefore, we freeze the parameters of Koopa but only use the incoming ground truth for operator adaptation of \(K_{\text{var}}\) in Time-variant KPs.

### Implementation of Operator Adaptation

At the beginning of rolling forecast, the Encoder in Time-variant KP outputs \(D\)-dimensional Koopman embedding for each observed series segment as \([z_{1},z_{2},\ldots,z_{F}]\), where \(F=\frac{H_{\text{tr}}}{S}\) is the segment number with \(S\) as the segment length. The operator \(K_{\text{var}}\) in Time-variant KP is calculated as follows:

\[Z_{\text{back}}=[z_{1},z_{2},\ldots,z_{F-1}],\;Z_{\text{fore}}=[z_{2},z_{3}, \ldots,z_{F}],\;K_{\text{var}}=Z_{\text{fore}}Z_{\text{back}}^{\dagger}, \tag{15}\]

where \(Z_{\text{back}},Z_{\text{fore}}\in\mathbb{R}^{D\times(F-1)},K_{\text{var}}\in \mathbb{R}^{D\times D}\). With the calculated operator, we obtain the next predicted Koopman embedding by one-step forwarding:

\[\hat{z}_{F+1}=K_{\text{var}}z_{F}. \tag{16}\]

After decoding the embedding \(\hat{z}_{F+1}\) to the series prediction, we can utilize the true value of incoming Koopman embedding \(z_{F+1}\) obtained by Koopa with frozen parameters. Instead of using \(K_{\text{var}}\) to obtain the next embedding\(\hat{z}_{F+2}\), we use incremental embedding collections \(Z_{\text{back}+},Z_{\text{fore}+}\in\mathbb{R}^{D\times P}\) to obtain a more accurate operator \(K_{\text{wt}+}\in\mathbb{R}^{D\times D}\) to describe the local dynamics:

\[Z_{\text{back}+}=[Z_{\text{back}},z_{F}],\ Z_{\text{fore}+}=[Z_{\text{back}},z_ {F+1}],\ K_{\text{wt}+}=Z_{\text{fore}+}Z_{\text{back}+}^{\dagger}. \tag{17}\]

The procedure repeats for \(L\) times (\(L\propto H_{\text{te}}\)) until the forecast horizon is all filled, we formulate it as Algorithm 1. And experimental results (Koopa OA) in Section 5.3 have demonstrated the promotion of forecasting performance due to more precisely fitted dynamics.

### Computational Acceleration

The naive implementation shown in Algorithm 1 repeatedly conducts Equation 17 on the incremental embedding collection to obtain new operators, which has a complexity of \(\mathcal{O}(LD^{3})\). We propose an equivalent algorithm with improved complexity of \(\mathcal{O}((L+D)D^{2})\) as shown in Algorithm 2.

**Theorem**.: _Algorithm 2 gives the same \(K_{\text{uw}}\) as Algorithm 1 in each iteration with \(\mathcal{O}(D^{2})\) complexity._

**Proof.** We start with the first iteration analysis. By the definition of Moore-Penrose inverse, we have \(Z_{\text{back}}^{\dagger}Z_{\text{back}}=I_{F-1}\), where \(I_{F-1}\) is an identity matrix with the dimension of \(F-1\). When the model receives the incoming embedding \(z_{F+1}\), incremental embedding \(m=z_{F},n=z_{F+1}\) will be appended to \(Z_{\text{back}}\) and \(Z_{\text{fore}}\) respectively. Instead of calculating new \(K_{\text{var}+}\) from incremental collections, we utilize calculated \(K_{\text{var}}\) to find the iteration rule on \(K_{\text{var}}\). Concretely, we suppose

\[Z_{\text{back}+}^{\dagger}=\begin{bmatrix}Z_{\text{back}}^{\dagger}-\Delta\\ b^{\top}\end{bmatrix}\in\mathbb{R}^{P\times D}, \tag{18}\]

where \(\Delta\in\mathbb{R}^{(F-1)\times D},b\in\mathbb{R}^{D}\) are variables to be identified. By the definition of Moore-Penrose inverse, we have \(Z_{\text{back}+}^{\dagger}Z_{\text{back}+}=I_{F}\). By unfolding it, we have the following equations:

\[\Delta Z_{\text{back}}=\mathbf{0},\ b^{\top}Z_{\text{back}}=\vec{0}\cdot b^{ \top}m=1,\ Z_{\text{back}}^{\dagger}m-\Delta m=\vec{0}. \tag{19}\]

We suppose \(\Delta=\delta b^{\top}\), where \(\delta\in\mathbb{R}^{P-1}\), such that when \(b^{\top}Z_{\text{back}}=\vec{0}\), then \(\Delta Z_{\text{back}}=\mathbf{0}\). Then we have \(Z_{\text{back}}^{\dagger}m-\delta b^{\top}m=Z_{\text{back}}^{\dagger}m-\delta =\vec{0}\), thus \(\Delta=Z_{\text{back}}^{\dagger}mb^{\top}\). Given equations that \(b^{\top}Z_{\text{back}}=\vec{0}\) and \(b^{\top}m=1\), we have the analytical solution of \(b\):

\[b=r/||r||^{2}\text{, where }r=m-Z_{\text{back}}Z_{\text{back}}^{\dagger}m. \tag{20}\]

Therefore, we find the equation between the incremental version \(K_{\text{var}+}\) and calculated \(K_{\text{var}}\):

\[Z_{\text{back}+}^{\dagger}=\begin{bmatrix}Z_{\text{back}}^{\dagger}(I_{D}-mb^ {\top})\\ b^{\top}\end{bmatrix},\ K_{\text{var}+}=Z_{\text{fore}+}Z_{\text{back}+}^{ \dagger}=K_{\text{var}}+(n-K_{\text{var}}m)b^{\top}, \tag{21}\]

where \(m,n\) are the incremental embedding of \(Z_{\text{back}}\), \(Z_{\text{fore}}\) and \(b\) can be calculated by Equation 20. We also derive the iteration rule on \(X=Z_{\text{back}}Z_{\text{back}}^{\dagger}\) to obtain \(b\), which is formulated as follows:

\[X_{+}=Z_{\text{back}*}Z_{\text{back}*}^{\dagger}=X+(m-Xm)b^{\top}=X+rb^{\top}. \tag{22}\]

By adopting Equation 21- 22 and permuting the matrix multiplication order, we reduce the complexity of each iteration to \(\mathcal{O}(D^{2})\). Therefore, Algorithm 2 has a overall complexity of \(\mathcal{O}((L+D)D^{2})\). Since \(L\propto H_{\text{te}}\), Algorithm 1-2 have \(\mathcal{O}(H_{\text{te}}D^{3})\) and \(\mathcal{O}((H_{\text{te}}+D)D^{2})\) complexity respectively.

## Appendix B Implementation Details

Koopa is trained with L2 loss and optimized by ADAM [17] with an initial learning rate of 0.001 and batch size set to 32. The training process is early stopped within 10 epochs. We repeat each experiment three times with different random seeds to obtain average test MSE/MAE and detailed results with standard deviations are listed in Table 6. Experiments are implemented in PyTorch [34] and conducted on NVIDIA TITAN RTX 24GB GPUs.

All the baselines that we reproduced are implemented based on the benchmark of TimesNet [47] Repository, which is fairly built on the configurations provided by each model's original paper or official code. Since several baselines adopt Series Stationarization from Non-stationary Transformers [28] while others do not, we equip all models with the method for a fair comparison.

## Appendix C Hyperparameter Sensitivity

Considering the efficiency of hyperparameters search, we fix the segment length \(S=T/2\) and the number of Koopa blocks \(B=3\) in all our experiments. We verify the robustness of Koopa of other hyperparameters as ```
0: Observed embedding \(Z=[z_{1},\ldots,z_{F}]\) and successively incoming ground truth embedding \([z_{F+1},\ldots,z_{F+L}]\) with each embedding \(z_{i}\in\mathbb{R}^{D}\).
1:\(Z_{\text{back}}=[z_{1},\ldots,z_{F-1}]\), \(Z_{\text{fore}}=[z_{2},\ldots,z_{F}]\)\(\triangleright\)\(Z_{\text{back}},Z_{\text{fore}}\in\mathbb{R}^{D\times(F-1)}\)
2:\(K_{\text{var}}=Z_{\text{fore}}Z_{\text{back}}^{\dagger}\)\(\triangleright\)\(K_{\text{var}}\in\mathbb{R}^{D\times D}\)
3:\(\hat{z}_{F+1}=K_{\text{var}}n\)\(\triangleright\)\(\hat{z}_{F+1}\in\mathbb{R}^{D}\)
4:for\(l\)in\(\{1,\ldots,L\}\):\(\triangleright\)\(z_{F+l}\) comes successively
5:\(m=z_{F+l-1},n=z_{F+l}\)\(\triangleright\)\(m,n\in\mathbb{R}^{D}\)
6:\(Z_{\text{back}}\leftarrow[Z_{\text{back}},m],Z_{\text{fore}}\leftarrow[Z_{ \text{fore}},n]\)\(\triangleright\)\(Z_{\text{back}},Z_{\text{fore}}\in\mathbb{R}^{D\times(F+l-1)}\)
7:\(K_{\text{var}}=Z_{\text{fore}}Z_{\text{back}}^{\dagger}\)\(\triangleright\)\(K_{\text{var}}\in\mathbb{R}^{D\times D}\)
8:\(\hat{z}_{F+l+1}=K_{\text{var}}n\)\(\triangleright\)\(\hat{z}_{F+l+1}\in\mathbb{R}^{D}\)
9:End for
10:Return\([\hat{z}_{F+1},\ldots,\hat{z}_{F+L+1}]\)\(\triangleright\) Return predicted embedding
```

**Algorithm 1** Kappa Operator Adaptation.

**Require:** Observed embedding \(Z=[z_{1},\ldots,z_{F}]\) and successively incoming ground truth embedding \([z_{F+1},\ldots,z_{F+L}]\) with each embedding \(z_{i}\in\mathbb{R}^{D}\).

1:\(Z_{\text{back}}=[z_{1},\ldots,z_{F-1}]\), \(Z_{\text{fore}}=[z_{2},\ldots,z_{F}]\)\(\triangleright\)\(Z_{\text{back}},Z_{\text{fore}}\in\mathbb{R}^{D\times(F-1)}\)
2:\(K_{\text{var}}=Z_{\text{fore}}Z_{\text{back}}^{\dagger},X=Z_{\text{back}}Z_{ \text{back}}^{\dagger}\)\(\triangleright\)\(K_{\text{var}},X\in\mathbb{R}^{D\times D}\)
3:\(\hat{z}_{F+1}=K_{\text{var}}n\)\(\triangleright\)\(\hat{z}_{F+1}\in\mathbb{R}^{D}\)
4:for\(l\)in\(\{1,\ldots,L\}\):\(\triangleright\)\(z_{F+l}\) comes successively
5:\(m=z_{F+l-1},n=z_{F+l}\)\(\triangleright\)\(m,n\in\mathbb{R}^{D}\)
6:\(r=m-Xm\)\(\triangleright\)\(r\in\mathbb{R}^{D}\)
7:\(b=r/||r||^{2}\)\(\triangleright\)\(b\in\mathbb{R}^{D}\)
8:\(K_{\text{var}}\gets K_{\text{var}}+(n-K_{\text{var}}m)b^{\top}\)\(\triangleright\)\(K_{\text{var}}\in\mathbb{R}^{D\times D}\)
9:\(X\gets X+rb^{\top}\)\(\triangleright\)\(X\in\mathbb{R}^{D\times D}\)
10:\(\hat{z}_{F+l+1}=K_{\text{var}}n\)\(\triangleright\)\(\hat{z}_{F+l+1}\in\mathbb{R}^{D}\)
11:End for
12:Return\([\hat{z}_{F+1},\ldots,\hat{z}_{F+L+1}]^{\top}\)\(\triangleright\) Return predicted embedding ```

**Algorithm 2** Accelerated Koopa Operator Adaptation.

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c|}{ECL} & \multicolumn{2}{c|}{ETTh2} & \multicolumn{2}{c}{Exchange} \\ \cline{2-7} Horizon & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \(H_{1}\) & 0.130\(\pm\)0.003 & 0.234\(\pm\)0.003 & 0.226\(\pm\)0.003 & 0.300\(\pm\)0.003 & 0.042\(\pm\)0.002 & 0.143\(\pm\)0.003 \\ \(H_{2}\) & 0.136\(\pm\)0.004 & 0.236\(\pm\)0.005 & 0.297\(\pm\)0.004 & 0.349\(\pm\)0.004 & 0.083\(\pm\)0.004 & 0.207\(\pm\)0.004 \\ \(H_{3}\) & 0.149\(\pm\)0.003 & 0.247\(\pm\)0.003 & 0.333\(\pm\)0.004 & 0.381\(\pm\)0.003 & 0.130\(\pm\)0.005 & 0.261\(\pm\)0.003 \\ \(H_{4}\) & 0.156\(\pm\)0.004 & 0.254\(\pm\)0.003 & 0.356\(\pm\)0.005 & 0.393\(\pm\)0.004 & 0.184\(\pm\)0.009 & 0.309\(\pm\)0.005 \\ \hline Dataset & \multicolumn{2}{c|}{ILI} & \multicolumn{2}{c|}{Traffic} & \multicolumn{2}{c}{Weather} \\ \cline{2-7} Horizon & MSE & MAE & MSE & MAE & MSE & MAE \\ \(H_{1}\) & 1.621\(\pm\)0.008 & 0.800\(\pm\)0.006 & 0.415\(\pm\)0.003 & 0.274\(\pm\)0.005 & 0.126\(\pm\)0.005 & 0.168\(\pm\)0.004 \\ \(H_{2}\) & 1.803\(\pm\)0.040 & 0.855\(\pm\)0.020 & 0.401\(\pm\)0.005 & 0.275\(\pm\)0.004 & 0.154\(\pm\)0.006 & 0.205\(\pm\)0.003 \\ \(H_{3}\) & 1.768\(\pm\)0.015 & 0.903\(\pm\)0.008 & 0.397\(\pm\)0.004 & 0.276\(\pm\)0.003 & 0.172\(\pm\)0.005 & 0.225\(\pm\)0.005 \\ \(H_{4}\) & 1.743\(\pm\)0.040 & 0.891\(\pm\)0.009 & 0.403\(\pm\)0.007 & 0.284\(\pm\)0.009 & 0.193\(\pm\)0.003 & 0.241\(\pm\)0.004 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Detailed performance of Koopa. We report the MSE/MAE and standard deviation of different forecast horizons \(\{H_{1},H_{2},H_{3},H_{4}\}=\{24,36,48,60\}\) for ILI and \(\{48,96,144,192\}\) for others.

follows: the dimension of Koopman embedding \(D\), the hidden layer number \(l\) and the hidden dimension \(d\) used in Encoder and Decoder. As is shown in Figure 7, we find the proposed model is insensitive to the choices of above hyperparameters, which can be beneficial for practitioners to reduce tuning burden.

Intuitively, a larger dimension of Koopman embedding \(D\) can bring about a lower approximation error. We further dive into it and find that stacking blocks can enhance the model capacity, and thus the performance is insensitive to \(D\) when the model is deep enough. To address the concern, we further check the sensitivity of \(D\) under varying block number \(B\) in Figure 8. It can be seen that a larger \(B\) generally leads to lower error even if \(D\) is small. And the performance can be sensitive to \(D\) when the model is not deep enough (\(B=1,2\)).

Besides, we find the proposed model can be insensitive to \(S\) on several datasets while sensitive on ECL and Traffic datasets (The difference is about 10%). There are many variables in these two datasets, but our current design shares the \(S\) for all variables. Since different variables with distinct evolution periods implicitly require different optimal \(S\), the performance of the dataset with more variables is more likely to be influenced by \(S\). Therefore, we set \(S=T/2\) with relatively small performance fluctuation to deal with most situations.

## Appendix D Supplementary Experimental Results

### Full Forecasting Results

Due to the limited pages, we list additional multivariate benchmarks on ETT datasets [53] in Table 7, which includes the hourly recorded ETTh2 and 15-minutely recorded ETTm1/ETTm2, and the full univariate results of M4 [39] in Table 8, which contains the yearly, quarterly and monthly collected univariate marketing data. Notably, Koopa still achieves competitive performance compared with state-of-the-art deep forecasting models and specialized univariate models.

### Full Ablation Results

We elaborately conduct model ablations to verify the necessity of each proposed module: Time-invariant KP, Time-variant KP, Fourier Filter and evaluate alternative choices to disentangle dynamics. As shown in Table 9, Koopa conducts effective disentanglement and tackles the right dynamics with complementary KPs, and thus achieves the best forecasting performance.

Figure 8: Left: Hyperparameter sensitivity of the dimension of Koopman embedding under different settings of the block number. Right: Hyperparameter sensitivity of the segment length.

Figure 7: Left: Hyperparameter sensitivity with respect to the dimension of Koopman embedding, hidden layer number, and hidden dimension of Encoder and Decoder in Koopa.

### Model Efficiency

We comprehensively compare the forecasting performance, training speed, and memory footprint of our model with well-acknowledged deep forecasting models. The results are recorded with the official model configuration and the same batch size. We visualize the model efficiency under all six multivariate datasets in Figure 9- 11. In detail, compared with the previous state-of-the-art model PatchTST [8], Koopa consumes only 15.2% training time and 3.6% memory footprint respectively in ECL, 37.8% training time and 26.8% memory in ETH2, 23.5% training time and 37.3% memory in Exchange, 50.9% training time and 47.8% memory in ILI, 3.5% training time and 2.9% memory in Traffic, and 5.4% training time and 25.4% memory in Weather, leading to the averaged **77.3%** and **76.0%** saving of training time and memory footprint in all six datasets. The remarkable efficiency can be attributed to Koopa with MLPs as the building blocks, and we find the budget saving becoming more significant on datasets with more series variables (ECL, Traffic).

Besides, as an efficient linear model, the performance of Koopa still surpasses other MLP-based models. Especially, Compared with DLinear [51], our model reduces 38.0% MSE (2.852\(\rightarrow\)1.768) in ILI and 13.6% MSE (0.452\(\rightarrow\)0.397) in Weather. And the average MSE reduction of Koopa compared with the previous state-of-the-art MLP-based model reaches **12.2%**. Therefore, our proposed Koopa is efficiently built with MLP networks and shows great model capacity to exploit nonlinear dynamics and complicated temporal dependencies in real-world time series.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{**KoopA**} & \multirow{2}{*}{**PatchTST [31]**} & \multirow{2}{*}{**TimesNet [47]**} & \multirow{2}{*}{**ILinear [51]**} & \multirow{2}{*}{**MICN [43]**} & \multirow{2}{*}{**KNF [44]**} & \multirow{2}{*}{**FiLM [54]**} & \multirow{2}{*}{**Autoformer [48]**} \\ \cline{4-4} \cline{8-14

\begin{table}
\begin{tabular}{c|c|c c|c c|c c|c c|c c} \hline \hline \multicolumn{2}{c}{Models} & \multicolumn{2}{c}{**KooPA**} & \multicolumn{2}{c}{Only \(K_{\text{inv}}\)} & \multicolumn{2}{c}{Only \(K_{\text{var}}\)} & \multicolumn{2}{c}{Truncated Filter} & \multicolumn{2}{c}{Branch Switch} \\ \cline{3-11} \multicolumn{2}{c}{Metric} & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{4}{*}{\(K_{\text{var}}\)} & 48 & **0.130** & **0.234** & 0.150 & 0.243 & 1.041 & 0.777 & 0.149 & 0.245 & 0.137 & 0.234 \\  & 96 & **0.136** & **0.236** & 0.137 & 0.242 & 4.643 & 1.669 & 0.172 & 0.280 & 2.240 & 0.724 \\  & 144 & **0.149** & 0.247 & 0.150 & 0.252 & 0.238 & 0.327 & **0.149** & **0.246** & 0.226 & 0.331 \\  & 192 & 0.156 & 0.254 & 0.158 & 0.260 & 0.267 & 0.355 & **0.152** & **0.248** & 0.181 & 0.284 \\ \hline \multirow{4}{*}{\(K_{\text{var}}\)} & 48 & **0.226** & **0.300** & 0.235 & 0.304 & 0.271 & 0.334 & 0.340 & 0.310 & 0.245 & 0.317 \\  & 96 & **0.297** & **0.349** & 0.311 & 0.353 & 0.382 & 0.405 & 0.301 & 0.352 & 0.343 & 0.384 \\  & 144 & **0.333** & 0.381 & 0.337 & **0.379** & 0.427 & 0.444 & 0.338 & 0.386 & 0.403 & 0.418 \\  & 192 & **0.356** & **0.393** & 0.363 & 0.397 & 0.402 & 0.437 & 0.363 & 0.400 & 0.384 & 0.420 \\ \hline \multirow{4}{*}{\(K_{\text{var}}\)} & 48 & **0.042** & **0.143** & 0.046 & 0.150 & 0.065 & 0.184 & 0.048 & 0.150 & 0.055 & 0.165 \\  & 96 & **0.083** & **0.207** & **0.083** & 0.210 & 0.147 & 0.274 & 0.087 & 0.210 & 0.151 & 0.277 \\  & 144 & **0.130** & **0.261** & 0.149 & 0.281 & 0.222 & 0.351 & 0.150 & 0.278 & 0.254 & 0.369 \\  & 192 & **0.184** & **0.309** & 0.200 & 0.322 & 0.385 & 0.456 & 0.229 & 0.345 & 0.463 & 0.490 \\ \hline \multirow{4}{*}{\(K_{\text{var}}\)} & 24 & **1.621** & **0.800** & 2.165 & 0.882 & 1.972 & 0.919 & 2.140 & 0.874 & 2.092 & 0.894 \\  & 36 & **1.803** & 0.855 & 1.815 & 0.882 & 2.675 & 1.091 & 1.692 & **0.844** & 2.116 & 0.950 \\  & 48 & **1.768** & 0.903 & 2.107 & 0.981 & 2.446 & 1.045 & 1.762 & **0.895** & 2.394 & 1.084 \\  & 60 & **1.743** & **0.891** & 2.496 & 1.108 & 2.387 & 0.970 & 2.357 & 1.018 & 1.917 & 0.926 \\ \hline \multirow{4}{*}{\(K_{\text{var}}\)} & 48 & **0.415** & **0.274** & 0.445 & 0.295 & 0.915 & 0.536 & 0.668 & 0.363 & 0.468 & 0.300 \\  & 96 & **0.401** & **0.275** & 0.403 & 0.277 & 0.833 & 0.465 & 0.441 & 0.323 & 0.429 & 0.298 \\  & 144 & **0.397** & **0.276** & 0.400 & 0.278 & 0.816 & 0.452 & 0.436 & 0.321 & 0.438 & 0.307 \\  & 192 & **0.403** & **0.284** & 1.371 & 0.788 & 1.224 & 0.723 & 0.597 & 0.331 & 0.469 & 0.312 \\ \hline \multirow{4}{*}{\(K_{\text{var}}\)} & 48 & 0.126 & 0.168 & 0.142 & 0.181 & 0.140 & 0.190 & **0.125** & **0.166** & 0.130 & 0.173 \\  & 96 & 0.154 & 0.205 & 0.164 & 0.209 & 0.169 & 0.224 & **0.154** & **0.202** & 0.163 & 0.210 \\ \cline{1-1}  & 144 & **0.172** & **0.225** & 0.178 & 0.226 & 0.194 & 0.247 & 0.176 & 0.226 & 0.187 & 0.238 \\ \cline{1-1}  & 192 & **0.193** & **0.241** & 0.195 & 0.245 & 0.217 & 0.268 & 0.195 & 0.244 & 0.212 & 0.261 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Model ablation with detailed forecasting performance. We report forecasting results with different prediction lengths \(\{24,36,48,60\}\) for ILI and \(H\in\{48,96,144,192\}\) for others. For columns: Only \(K_{\text{inv}}\) uses one-block Time-invariant KP; Only \(K_{\text{var}}\) stacks Time-variant KPs only; _Truncated Filter_ replaces Fourier Filter with High-Low Frequency Pass Filter; _Branch Switch_ changes the order of KPs to deal with disentangled components.

Figure 9: Model efficiency comparison with forecast length \(H=144\) for ETH2 and Traffic.

### Training Stability

Since the eigenvalues of the operator determine the amplitude of dynamics evolution, where modulus not close to one causes non-divergent and explosive evolution in the long-term, we highlight that a stable Koopman operator only describes weakly stationary series well. While most of the Koopman-based forecasters can suffer from the operator convergence problem induced by complicated non-stationary series variations, we employ several techniques to stabilize the training process.

Operator initializationWe adopt the operator initialization strategy, where the operator starts from the multiplication of eigenfunctions with standard Gaussian distribution and all-one eigenvalues.

Hierarchical disentanglementIn our model, each block learns weak stationary process hierarchically and feeds the residual of fitted dynamics for the next block to correct. Thus Koopman Predictor aims not to fully reconstruct the whole dynamics at once, but to partially describe dynamics, so rigorous reconstruction is not forced in each block, reducing the difficulty of portraying the non-stationary series as dynamics.

Explosion checkingWe introduce an explosion checking mechanism that replaces the operator encountering nan number with the identity matrix when the exponential multiplication of multiple time steps is detected.

Based on the proposed strategies, we provide the model training curves in Figure 12 to check the convergence of our proposed model and other forecasters. The training curves of the proposed model in blue demonstrate a consistent and smooth convergence, indicating its effectiveness in converging toward an optimal solution.

## Appendix E Broader Impact

### Impact on Real-world Applications

Our work copes with real-world time series forecasting, which is faced with intrinsic non-stationarity that poses fundamental challenges for deep forecasting models. Since previous studies hardly research the theoretical basis that can naturally address the time-variant property in non-stationary data, we propose a novel Koopman

Figure 11: Model efficiency comparison with forecast length \(H=144\) for Weather and \(48\) for ILI.

Figure 10: Model efficiency comparison with forecast length \(H=144\) for Exchange and ECL.

forecaster that fundamentally considers the implicit time-variant and time-invariant dynamics based on Koopman theory. Our model achieves the state-of-the-art performance on six real-world forecasting tasks, covering energy, economics, disease, traffic, and weather, and demonstrates remarkable model efficiency in training time and memory footprint. Therefore, the proposed model makes it promising to tackle real-world forecasting applications, which helps our society prevent risks in advance and make better decisions with limited computational budgets. Our paper mainly focuses on scientific research and has no obvious negative social impact.

### Impact on Future Research

In this paper, we find modern Koopman theory natural to learn the dynamics underlying non-stationary time series. The proposed model explores complex non-stationary patterns with temporal localization inspired by Koopman approaches and implements respective deep network modules to disentangle and portray time-variant and time-invariant dynamics with the enlightenment of Wold's Theorem. The remarkable efficiency and insights from the theory can be instructive for future research.

## Appendix F Limitation

Our proposed model does not respectively considers dynamics in different variates, which leaves improvement for better multivariate forecasting with the consideration of various evolution patterns and series relationships. And Koopman spectral theory is still under leveraging in our work, which can discover Koopman modes to interpret the linear behavior underlying non-stationary data in a high-dimensional representation. Besides, Koopman theory for control considering factors outside the system can be promising for series forecasting with covariates, which leaves our future work.

Figure 12: Training curves of different models on the ETT and Weather datasets.